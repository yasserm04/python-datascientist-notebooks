{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c73f71-ad68-4d60-a98b-673c70604924",
   "metadata": {},
   "source": [
    "# Manipuler des donn√©es avec Pandas\n",
    "\n",
    "Lino Galiana  \n",
    "2025-10-06\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">Pour essayer les exemples pr√©sents dans ce tutoriel :</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/manipulation/02_pandas_suite.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=¬´02_pandas_suite¬ª&init.personalInit=¬´https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh¬ª&init.personalInitArgs=¬´manipulation%2002_pandas_suite%20correction¬ª\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=¬´02_pandas_suite¬ª&init.personalInit=¬´https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh¬ª&init.personalInitArgs=¬´manipulation%2002_pandas_suite%20correction¬ª\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//blob/main//notebooks/manipulation/02_pandas_suite.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> Ceci est la version fran√ßaise üá´üá∑ de ce chapitre, pour voir la version anglaise allez <a href=\"/home/runner/work/python-datascientist/python-datascientist/en/content/manipulation/02_pandas_suite.qmd\">ici</a>.\n",
    "\n",
    "> **Comp√©tences √† l‚Äôissue de ce chapitre**\n",
    ">\n",
    "> -   Savoir construire des statistiques agr√©g√©es fines gr√¢ce aux m√©thodes de `Pandas` ;\n",
    "> -   Savoir restructurer ses donn√©es et joindre plusieurs `DataFrames` ensemble ;\n",
    "> -   Cr√©er des tableaux attractifs pour communiquer sur des r√©sultats agr√©g√©s ;\n",
    "> -   Conna√Ætre les limites de `Pandas` et les *packages* alternatifs.\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "Le [chapitre d‚Äôintroduction √† `Pandas`](../../content/manipulation/02_pandas_intro.qmd) a permis de pr√©senter le principe de donn√©es organis√©es sous une forme de *DataFrame* et la praticit√© de l‚Äô√©cosyst√®me `Pandas` pour effectuer des op√©rations simples sur un jeu de donn√©es.\n",
    "\n",
    "Il est rare de travailler exclusivement sur une source brute. Un jeu de donn√©es prend g√©n√©ralement de la valeur lorsqu‚Äôil est compar√© √† d‚Äôautres sources. Pour des chercheurs, cela permettra de contextualiser l‚Äôinformation pr√©sente dans une source en la comparant ou en l‚Äôassociant √† d‚Äôautres sources. Pour des *data scientists* dans le secteur priv√©, il s‚Äôagira souvent d‚Äôassocier des informations sur une m√™me personne dans plusieurs bases clientes ou comparer les clients entre eux.\n",
    "\n",
    "L‚Äôun des apports des outils modernes de *data science*, notamment `Pandas` est la simplicit√© par laquelle ils permettent de restructurer des sources pour travailler sur plusieurs donn√©es sur un projet.\n",
    "Ce chapitre consolide ainsi les principes vus pr√©c√©demment en raffinant les traitements faits sur les donn√©es. Il va explorer principalement deux types d‚Äôop√©rations:\n",
    "\n",
    "-   les statistiques descriptives par groupe ;\n",
    "-   l‚Äôassociation de donn√©es par des caract√©ristiques communes.\n",
    "\n",
    "Effectuer ce travail de mani√®re simple, fiable et efficace est indispensable pour les *data scientists* tant cette t√¢che est courante. Heureusement `Pandas` permet de faire cela tr√®s bien avec des donn√©es structur√©es. Nous verrons dans les prochains chapitres, mais aussi dans l‚Äôensemble de la [partie sur le traitement des donn√©es textuelles](../../content/nlp/index.qmd), comment faire avec des donn√©es moins structur√©es.\n",
    "\n",
    "Gr√¢ce √† ce travail, nous allons approfondir notre compr√©hension d‚Äôun ph√©nom√®ne r√©el par le biais de statistiques descriptives fines. Cela est une √©tape indispensable avant de basculer vers la [statistique inf√©rentielle](https://fr.wikipedia.org/wiki/Inf%C3%A9rence_statistique#:~:text=L'inf%C3%A9rence%20statistique%20est%20l,%3A%20la%20probabilit%C3%A9%20d'erreur.), l‚Äôapproche qui consiste √† formaliser et g√©n√©raliser des liens de corr√©lation ou de causalit√© entre des caract√©ristiques observ√©es et un ph√©nom√®ne.\n",
    "\n",
    "> **Comp√©tences √† l‚Äôissue de ce chapitre**\n",
    ">\n",
    "> -   R√©cup√©rer un jeu de donn√©es officiel de l‚ÄôInsee ;\n",
    "> -   Construire des statistiques descriptives par groupe et jongler entre les niveaux des donn√©es ;\n",
    "> -   Associer des donn√©es (*reshape*, *merge*) pour leur donner plus de valeur ;\n",
    "> -   Faire un beau tableau pour communiquer des statistiques descriptives.\n",
    "\n",
    "## 1.1 Environnement\n",
    "\n",
    "Le chapitre pr√©c√©dent utilisait quasi exclusivement la librairie `Pandas`. Nous allons dans ce chapitre utiliser d‚Äôautres *packages* en compl√©ment de celui-ci.\n",
    "\n",
    "Comme expliqu√© ci-dessous, nous allons utiliser une librairie nomm√©e `pynsee` pour r√©cup√©rer les donn√©es de l‚ÄôInsee utiles √† enrichir notre jeu de donn√©es de l‚ÄôAdeme. Cette librairie n‚Äôest pas install√©e par d√©faut dans `Python`. Avant de pouvoir l‚Äôutiliser,\n",
    "il est n√©cessaire de l‚Äôinstaller, comme la librairie `great_tables` que nous verrons √† la fin de ce chapitre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddaa868",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd --quiet\n",
    "!pip install pynsee --quiet\n",
    "!pip install great_tables --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c27ea5-ebc5-4c7f-bb7f-bfc79653b264",
   "metadata": {},
   "source": [
    "L‚Äôinstruction `!pip install <pkg>` est une mani√®re de faire comprendre √† `Jupyter`, le moteur d‚Äôex√©cution derri√®re les *notebooks* que la commande qui suit (`pip install` ce `<pkg>`)\n",
    "est une commande syst√®me, √† ex√©cuter hors de `Python` (dans le terminal par exemple pour un syst√®me `Linux`).\n",
    "\n",
    "Les premiers *packages* indispensables pour d√©marrer ce chapitre sont les suivants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a966414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pynsee\n",
    "import pynsee.download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312bb09-f365-4b5d-9cb9-680bcc1a71d6",
   "metadata": {},
   "source": [
    "Pour obtenir des r√©sultats reproductibles, on peut fixer la racine du g√©n√©rateur\n",
    "pseudo-al√©atoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3163f32-24d5-4127-a63b-24dc2e2a13bf",
   "metadata": {},
   "source": [
    "## 1.2 Donn√©es utilis√©es\n",
    "\n",
    "Ce tutoriel continue l‚Äôexploration du jeu de donn√©es du chapitre pr√©c√©dent:\n",
    "\n",
    "-   Les √©missions de gaz √† effet de serre estim√©es au niveau communal par l‚ÄôADEME. Le jeu de donn√©es est\n",
    "    disponible sur [data.gouv](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_)\n",
    "    et requ√™table directement dans `Python` avec\n",
    "    [cet url](https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert) ;\n",
    "\n",
    "Les probl√©matiques d‚Äôenrichissement de donn√©es (association d‚Äôune source √† une autre √† partir de caract√©ristiques communes) seront pr√©sent√©es √† partir de deux sources produites par l‚ÄôInsee:\n",
    "\n",
    "-   Le\n",
    "    [code officiel g√©ographique](https://www.insee.fr/fr/statistiques/fichier/6800675/v_commune_2023.csv),\n",
    "    un r√©f√©rentiel\n",
    "    produit par l‚ÄôInsee utilis√© pour identifier les communes √† partir d‚Äôun code unique, contrairement au code postal ;\n",
    "-   Les donn√©es [*Filosofi*](https://www.insee.fr/fr/metadonnees/source/serie/s1172), une source sur les revenus des Fran√ßais √† une √©chelle spatiale fine construite par l‚ÄôInsee √† partir des d√©clarations fiscales et d‚Äôinformations sur les prestations sociales. En l‚Äôoccurrence, nous allons utiliser les niveaux de revenu et les populations[1] au niveau communal afin de les mettre en regard de nos donn√©es d‚Äô√©missions.\n",
    "\n",
    "Pour faciliter l‚Äôimport de donn√©es Insee, il est recommand√© d‚Äôutiliser le *package*\n",
    "[`pynsee`](https://pynsee.readthedocs.io/en/latest/) qui simplifie l‚Äôacc√®s aux principaux jeux de donn√©es\n",
    "de l‚ÄôInsee disponibles sur le site web [insee.fr](https://www.insee.fr/fr/accueil)\n",
    "ou via des API.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> Le *package* `pynsee` comporte deux principaux points d‚Äôentr√©e :\n",
    ">\n",
    "> -   Les API de l‚ÄôInsee, ce qui sera illustr√© dans le chapitre consacr√©.\n",
    "> -   Quelques jeux de donn√©es directement issus du site web de\n",
    ">     l‚ÄôInsee ([insee.fr](https://www.insee.fr/fr/accueil))\n",
    ">\n",
    "> Dans ce chapitre, nous allons exclusivement utiliser cette deuxi√®me\n",
    "> approche. Cela se fera par le module `pynsee.download`.\n",
    ">\n",
    "> La liste des donn√©es disponibles depuis ce *package* est [ici](https://inseefrlab.github.io/DoReMIFaSol/articles/donnees_dispo.html).\n",
    "> La fonction `download_file` attend un identifiant unique\n",
    "> pour savoir quelle base de donn√©es aller chercher et\n",
    "> restructurer depuis le\n",
    "> site [insee.fr](https://www.insee.fr/fr/accueil).\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Conna√Ætre la liste des bases disponibles\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> Pour conna√Ætre la liste des bases disponibles, vous\n",
    "> pouvez utiliser la fonction `meta = pynsee.get_file_list()`\n",
    "> apr√®s avoir fait `import pynsee`.\n",
    "> Celle-ci renvoie un `DataFrame` dans lequel on peut\n",
    "> rechercher, par exemple gr√¢ce √† une recherche\n",
    "> de mots-clefs :\n",
    ">\n",
    "> ``` python\n",
    "> import pynsee\n",
    "> meta = pynsee.get_file_list()\n",
    "> meta.loc[meta['label'].str.contains(r\"Filosofi.*2016\")]\n",
    "> ```\n",
    ">\n",
    "> Ici, `meta['label'].str.contains(r\"Filosofi.*2016\")` signifie:\n",
    "> ‚Äú*`pandas` trouve moi tous les labels o√π sont contenus les termes Filosofi et 2016.*‚Äù\n",
    "> (`.*` signifiant ‚Äú*peu m‚Äôimporte le nombre de mots ou caract√®res entre*‚Äù)\n",
    ">\n",
    "> </details>\n",
    "\n",
    "# 2. R√©cup√©ration des jeux de donn√©es\n",
    "\n",
    "## 2.1 Donn√©es d‚Äô√©mission de l‚ÄôAdeme\n",
    "\n",
    "Comme expliqu√© au chapitre pr√©c√©dent, ces donn√©es peuvent √™tre import√©es tr√®s simplement avec `Pandas`\n",
    "\n",
    "[1] Ideally, it would be more coherent, for demographic data, to use the [legal populations](https://www.insee.fr/fr/information/2008354), from the census. However, this base is not yet natively integrated into the `pynsee` library that we will use in this chapter. An open exercise is proposed to construct population aggregates from anonymized individual census data (the [detailed files](https://www.insee.fr/fr/information/2383306))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f388ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\"\n",
    "emissions = pd.read_csv(url)\n",
    "emissions.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49ce96-bcec-40c7-b4f9-01cdf2e8e4e4",
   "metadata": {},
   "source": [
    "Nous allons d‚Äôores et d√©j√† conserver le nom des secteurs √©metteurs pr√©sents dans la base de donn√©es pour simplifier des utilisations ult√©rieures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83047446",
   "metadata": {},
   "outputs": [],
   "source": [
    "secteurs = emissions.select_dtypes(include='number').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f27752-2436-4efb-8a69-3b8845fc0331",
   "metadata": {},
   "source": [
    "Les exploitations ult√©rieures de ces donn√©es utiliseront la dimension d√©partementale dont nous avons montr√© la construction au chapitre pr√©c√©dent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions['dep'] = emissions[\"INSEE commune\"].str[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75829a9-b112-49c4-a448-653cddd57a44",
   "metadata": {},
   "source": [
    "## 2.2 Donn√©es *Filosofi*\n",
    "\n",
    "On va utiliser les donn√©es Filosofi (donn√©es de revenus) au niveau communal de 2016.\n",
    "Ce n‚Äôest pas la m√™me ann√©e que les donn√©es d‚Äô√©mission de CO2, ce n‚Äôest donc pas parfaitement rigoureux,\n",
    "mais cela permettra tout de m√™me d‚Äôillustrer\n",
    "les principales fonctionnalit√©s de `Pandas`\n",
    "\n",
    "Le point d‚Äôentr√©e principal de la fonction `pynsee` est la fonction `download_file`.\n",
    "\n",
    "Le code pour t√©l√©charger les donn√©es est le suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynsee.download import download_file\n",
    "filosofi = download_file(\"FILOSOFI_COM_2016\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef63624-ac6c-4a6f-8cac-fe5b3e65b880",
   "metadata": {},
   "source": [
    "Le *DataFrame* en question a l‚Äôaspect suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0def4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filosofi.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84c7dc-381f-4b9f-9c12-a8400fadebea",
   "metadata": {},
   "source": [
    "`Pandas` a g√©r√© automatiquement les types de variables. Il le fait relativement bien, mais une v√©rification est toujours utile pour les variables qui ont un statut sp√©cifique.\n",
    "\n",
    "Pour les variables qui ne sont pas en type `float` alors qu‚Äôelles devraient l‚Äô√™tre, on modifie leur type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91300967",
   "metadata": {},
   "outputs": [],
   "source": [
    "filosofi = (\n",
    "  filosofi\n",
    "  .astype(\n",
    "    {c: \"float\" for c in filosofi.columns[2:]}\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65826965-f884-4ab7-8034-634f13acb4b1",
   "metadata": {},
   "source": [
    "Un simple coup d‚Äôoeil sur les donn√©es\n",
    "donne une id√©e assez pr√©cise de la mani√®re dont les donn√©es sont organis√©es.\n",
    "On remarque que certaines variables de `filosofi` semblent avoir beaucoup de valeurs manquantes (secret statistique)\n",
    "alors que d‚Äôautres semblent compl√®tes.\n",
    "Si on d√©sire exploiter `filosofi`, il faut faire attention √† la variable choisie.\n",
    "\n",
    "Notre objectif √† terme va √™tre de relier l‚Äôinformation contenue entre ces\n",
    "deux jeux de donn√©es. En effet, sinon, nous risquons d‚Äô√™tre frustr√© : nous allons\n",
    "vouloir en savoir plus sur les √©missions de gaz carbonique mais seront tr√®s\n",
    "limit√©s dans les possibilit√©s d‚Äôanalyse sans ajout d‚Äôune information annexe\n",
    "issue de `filosofi`.\n",
    "\n",
    "# 3. Statistiques descriptives par groupe\n",
    "\n",
    "## 3.1 Principe\n",
    "\n",
    "Nous avons vu, lors du chapitre pr√©c√©dent, comment obtenir\n",
    "une statistique agr√©g√©e simplement gr√¢ce √† `Pandas`.\n",
    "Il est n√©anmoins commun d‚Äôavoir des donn√©es avec des strates\n",
    "interm√©diaires d‚Äôanalyse pertinentes: des variables g√©ographiques, l‚Äôappartenance √† des groupes socio-d√©mographiques li√©s √† des caract√©ristiques renseign√©es, des indicatrices de p√©riode temporelle, etc.\n",
    "Pour mieux comprendre la structure de ses donn√©es, les *data scientists* sont donc souvent amen√©s √† construire des statistiques descriptives sur des sous-groupes pr√©sents dans les donn√©es. Pour reprendre l‚Äôexemple sur les √©missions, nous avions pr√©c√©demment construit des statistiques d‚Äô√©missions au niveau national. Mais qu‚Äôen est-il du profil d‚Äô√©mission des diff√©rents d√©partements ? Pour r√©pondre √† cette question, il sera utile d‚Äôagr√©ger nos donn√©es au niveau d√©partemental. Ceci nous donnera une information diff√©rente du jeu de donn√©es initial (niveau communal) et du niveau le plus agr√©g√© (niveau national).\n",
    "\n",
    "En `SQL`, il est tr√®s simple de d√©couper des donn√©es pour\n",
    "effectuer des op√©rations sur des blocs coh√©rents et recollecter des r√©sultats\n",
    "dans la dimension appropri√©e.\n",
    "La logique sous-jacente est celle du *split-apply-combine* qui est repris\n",
    "par les langages de manipulation de donn√©es, auxquels `pandas`\n",
    "[ne fait pas exception](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html).\n",
    "\n",
    "L‚Äôimage suivante, issue de\n",
    "[ce site](https://unlhcc.github.io/r-novice-gapminder/16-plyr/),\n",
    "repr√©sente bien la mani√®re dont fonctionne l‚Äôapproche\n",
    "`split`-`apply`-`combine`:\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://unlhcc.github.io/r-novice-gapminder/fig/12-plyr-fig1.png\" alt=\"Split-apply-combine (Source: unlhcc.github.io)\" />\n",
    "<figcaption aria-hidden=\"true\">Split-apply-combine (Source: <a href=\"https://unlhcc.github.io/r-novice-gapminder/16-plyr/\">unlhcc.github.io</a>)</figcaption>\n",
    "</figure>\n",
    "\n",
    "En `Pandas`, on utilise `groupby` pour d√©couper les donn√©es selon un ou\n",
    "plusieurs axes (ce [tutoriel](https://realpython.com/pandas-groupby/) sur le sujet\n",
    "est particuli√®rement utile).\n",
    "L‚Äôensemble des op√©rations d‚Äôagr√©gation (comptage, moyennes, etc.) que nous avions vues pr√©c√©demment peut √™tre mise en oeuvre par groupe.\n",
    "\n",
    "Techniquement, cette op√©ration consiste √† cr√©er une association\n",
    "entre des labels (valeurs des variables de groupe) et des\n",
    "observations. Utiliser la m√©thode `groupby` ne d√©clenche pas d‚Äôop√©rations avant la mise en oeuvre d‚Äôune statistique, cela cr√©√© seulement une relation formelle entre des observations et des regroupemens qui seront utilis√©s *a posteriori*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384118b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filosofi[\"dep\"] = filosofi[\"CODGEO\"].str[:2]\n",
    "filosofi.groupby('dep').__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17768e3d-6a51-4829-b46e-6c5a0376b03a",
   "metadata": {},
   "source": [
    "Tant qu‚Äôon n‚Äôappelle pas une action sur un `DataFrame` par groupe, du type\n",
    "`head` ou `display`, `pandas` n‚Äôeffectue aucune op√©ration. On parle de\n",
    "*lazy evaluation*. Par exemple, le r√©sultat de `df.groupby('dep')` est\n",
    "une transformation qui n‚Äôest pas encore √©valu√©e :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767491ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "filosofi.groupby('dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97d0c4-1fc2-453c-a98b-210f0c92d062",
   "metadata": {},
   "source": [
    "## 3.2 Illustration 1: d√©nombrement par groupe\n",
    "\n",
    "Pour illustrer l‚Äôapplication de ce principe √† un comptage, on peut d√©nombrer le nombre de communes par d√©partement en 2023 (chaque ann√©e cette statistique change du fait des fusions de communes). Pour cela, il suffit de prendre le r√©f√©rentiel des communes fran√ßaises issu du code officiel g√©ographique (COG) et d√©nombrer par d√©partement gr√¢ce √† `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "url_cog_2023 = \"https://www.insee.fr/fr/statistiques/fichier/6800675/v_commune_2023.csv\"\n",
    "url_backup = \"https://minio.lab.sspcloud.fr/lgaliana/data/python-ENSAE/cog_2023.csv\"\n",
    "\n",
    "# Try-except clause to avoid timout issue sometimes\n",
    "# Without timeout problem, pd.read_csv(url_cog_2023) would be sufficient\n",
    "try:\n",
    "  response = requests.get(url_cog_2023)\n",
    "  response.raise_for_status()\n",
    "  cog_2023 = pd.read_csv(StringIO(response.text))\n",
    "except requests.exceptions.Timeout:\n",
    "  print(\"Failing back to backup\")\n",
    "  cog_2023 = pd.read_csv(url_backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03275fb3-8497-4177-a071-2cd4b4147256",
   "metadata": {},
   "source": [
    "Gr√¢ce √† ce jeu de donn√©es, sans avoir recours aux statistiques par groupe, on peut d√©j√† savoir combien on a, respectivement, de communes, d√©partements et r√©gions en France:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feceaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "communes = cog_2023.loc[cog_2023['TYPECOM']==\"COM\"]\n",
    "communes.loc[:, ['COM', 'DEP', 'REG']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574eb556-83b7-4317-acbb-e92e02b17ce1",
   "metadata": {},
   "source": [
    "Maintenant, int√©ressons nous aux d√©partements ayant le plus de communes. Il s‚Äôagit de la m√™me fonction de d√©nombrement o√π on joue, cette fois, sur le groupe √† partir duquel est calcul√© la statistique.\n",
    "\n",
    "Calculer cette statistique se fait de mani√®re assez transparente lorsqu‚Äôon conna√Æt le principe d‚Äôun calcul de statistiques avec `Pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7179886",
   "metadata": {},
   "outputs": [],
   "source": [
    "communes = cog_2023.loc[cog_2023['TYPECOM']==\"COM\"]\n",
    "communes.groupby('DEP').agg({'COM': 'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca36f4ec-4785-4a21-bd38-1d80d10cded6",
   "metadata": {},
   "source": [
    "En SQL, on utiliserait la requ√™te suivante:\n",
    "\n",
    "``` sql\n",
    "SELECT dep, COUNT DISTINCT \"COM\" AS COM\n",
    "FROM communes\n",
    "GROUP BY dep\n",
    "WHERE TYPECOM == 'COM';\n",
    "```\n",
    "\n",
    "La sortie est une `Serie` index√©e. Ce n‚Äôest pas tr√®s pratique comme nous avons pu l‚Äô√©voquer au cours du chapitre pr√©c√©dent. Il est plus pratique de transformer cet objet en `DataFrame` avec `reset_index`. Enfin, avec `sort_values`, on obtient la statistique d√©sir√©e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e77ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    communes\n",
    "    .groupby('DEP')\n",
    "    .agg({'COM': 'nunique'})\n",
    "    .reset_index()\n",
    "    .sort_values('COM', ascending = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2261d28-61b5-4efe-abc2-6efd0935c19a",
   "metadata": {},
   "source": [
    "## 3.3 Illustration 2: agr√©gats par groupe\n",
    "\n",
    "Pour illustrer les agr√©gats par groupe nous pouvons prendre le jeu de donn√©es de l‚ÄôInsee `filosofi` et compter la population gr√¢ce √† la variable `NBPERSMENFISC16`.\n",
    "\n",
    "Pour calculer le total au niveau France enti√®re nous pouvons faire de deux mani√®res :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filosofi['NBPERSMENFISC16'].sum()* 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ded60",
   "metadata": {},
   "outputs": [],
   "source": [
    "filosofi.agg({\"NBPERSMENFISC16\": \"sum\"}).div(1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7094f9-e418-4b63-b61d-eda6f7b22417",
   "metadata": {},
   "source": [
    "o√π les r√©sultats sont report√©s en millions de personnes. La logique est identique lorsqu‚Äôon fait des statistiques par groupe, il s‚Äôagit seulement de remplacer `filosofi` par `filosofi.groupby('dep')` pour cr√©er une version partitionn√©e par d√©partement de notre jeu de donn√©es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filosofi.groupby('dep')['NBPERSMENFISC16'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f123d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filosofi.groupby('dep').agg({\"NBPERSMENFISC16\": \"sum\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1126d26-b0c1-4f61-921b-38cef3088f62",
   "metadata": {},
   "source": [
    "La seconde approche est plus pratique car elle donne directement un `DataFrame` `Pandas` et non une s√©rie index√©e sans nom. A partir de celle-ci, quelques manipulations basiques peuvent suffire pour avoir un tableau diffusables sur la d√©mographie d√©partementale. N√©anmoins, celui-ci, serait quelques peu brut de d√©coffrage car nous ne poss√©dons √† l‚Äôheure actuelle que les num√©ros de d√©partement. Pour avoir le nom de d√©partements, il faudrait utiliser une deuxi√®me base de donn√©es et croiser les informations communes entre elles (en l‚Äôoccurrence le num√©ro du d√©partement). C‚Äôest l‚Äôobjet de la prochaine partie.\n",
    "\n",
    "## 3.4 Exercice d‚Äôapplication\n",
    "\n",
    "Cet exercice d‚Äôapplication s‚Äôappuie sur le jeu de donn√©es de l‚ÄôAdeme nomm√© `emissions` pr√©c√©demment.\n",
    "\n",
    "> **Exercice 1 : agr√©gations par groupe**\n",
    ">\n",
    "> 1.  Calculer les √©missions totales du secteur ‚ÄúR√©sidentiel‚Äù par d√©partement et rapporter la valeur au d√©partement le plus polluant dans le domaine. En tirer des intutitions sur la r√©alit√© que cette statistique refl√®te.\n",
    ">\n",
    "> 2.  Calculer, pour chaque d√©partement, les √©missions totales de chaque secteur. Pour chaque d√©partement, calculer la proportion des √©missions totales venant de chaque secteur.\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Indice pour cette question\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> -   *‚ÄúGrouper par‚Äù* = `groupby`\n",
    "> -   *‚Äú√©missions totales‚Äù* = `agg({*** : \"sum\"})`\n",
    ">\n",
    "> </details>\n",
    "\n",
    "A la question 1, le r√©sultat obtenu devrait √™tre le suivant:\n",
    "\n",
    "Ce classement refl√®te peut-√™tre plus la d√©mographie que le processus qu‚Äôon d√©sire mesurer. Sans l‚Äôajout d‚Äôune information annexe sur la population de chaque d√©partement pour contr√¥ler ce facteur, on peut difficilement savoir s‚Äôil y a une diff√©rence structurelle de comportement entre les habitants du Nord (d√©partement 59) et ceux de la Moselle (d√©partement 57).\n",
    "\n",
    "A l‚Äôissue de la question 2, prenons la part des √©missions de l‚Äôagriculture et du secteur tertiaire dans les √©missions d√©partementales:\n",
    "\n",
    "Ces r√©sultats sont assez logiques ; les d√©partements ruraux ont une part plus importante de leur √©mission issue de l‚Äôagriculture, les d√©partements urbains ont plus d‚Äô√©missions issues du secteur tertiaire, ce qui est li√© √† la densit√© plus importante de ces espaces.\n",
    "\n",
    "Gr√¢ce √† ces statistiques on progresse dans la connaissance de notre jeu de donn√©es et donc de la nature des √©missions de C02 en France.\n",
    "Les statistiques descriptives par groupe nous permettent de mieux saisir l‚Äôh√©t√©rog√©n√©it√© spatiale de notre ph√©nom√®ne.\n",
    "\n",
    "Cependant, on reste limit√© dans notre capacit√© √† interpr√©ter les statistiques obtenues sans recourir √† l‚Äôutilisation d‚Äôinformation annexe. Pour donner du sens et de la valeur √† une statistique, il faut g√©n√©ralement associer celle-ci √† de la connaissance annexe sous peine qu‚Äôelle soit d√©sincarn√©e.\n",
    "\n",
    "Dans la suite de ce chapitre, nous envisagerons une premi√®re voie qui est le croisement avec des donn√©es compl√©mentaires. On appelle ceci un enrichissement de donn√©es. Ces donn√©es peuvent √™tre des observations √† un niveau identique √† celui de la source d‚Äôorigine. Par exemple, l‚Äôun des croisements les plus communs est d‚Äôassocier une base client √† une base d‚Äôachats afin de mettre en regard un comportement d‚Äôachat avec des caract√©ristiques pouvant expliquer celui-ci. Les associations de donn√©es peuvent aussi se faire √† des niveaux conceptuels diff√©rents, en g√©n√©ral √† un niveau plus agr√©g√© pour contextualiser la donn√©e plus fine et comparer une observation √† des mesures dans un groupe similaire. Par exemple, on peut associer des temps et des modes de transports individuels √† ceux d‚Äôune m√™me classe d‚Äô√¢ge ou de personnes r√©sidant dans la m√™me commune pour pouvoir comparer la diff√©rence entre certains individus et un groupe sociod√©mographique similaire.\n",
    "\n",
    "# 4. Restructurer les donn√©es\n",
    "\n",
    "## 4.1 Principe\n",
    "\n",
    "Quand on a plusieurs informations pour un m√™me individu ou groupe, on\n",
    "retrouve g√©n√©ralement deux types de structure de donn√©es :\n",
    "\n",
    "-   format **wide** : les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu (ou groupe), dans des colonnes diff√©rentes\n",
    "-   format **long** : les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu, dans des lignes diff√©rentes avec une colonne permettant de distinguer les niveaux d‚Äôobservations\n",
    "\n",
    "Un exemple de la distinction entre les deux peut √™tre pris √† l‚Äôouvrage de r√©f√©rence d‚ÄôHadley Wickham, [*R for Data Science*](https://r4ds.hadley.nz/):\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://d33wubrfki0l68.cloudfront.net/3aea19108d39606bbe49981acda07696c0c7fcd8/2de65/images/tidy-9.png\" alt=\"Donn√©es long et wide (Source: R for Data Science)\" />\n",
    "<figcaption aria-hidden=\"true\">Donn√©es <em>long</em> et <em>wide</em> (Source: <a href=\"https://r4ds.hadley.nz/\"><em>R for Data Science</em></a>)</figcaption>\n",
    "</figure>\n",
    "\n",
    "L‚Äôaide m√©moire suivante aidera √† se rappeler les fonctions √† appliquer si besoin :\n",
    "\n",
    "![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/reshape.png)\n",
    "\n",
    "Le fait de passer d‚Äôun format *wide* au format *long* (ou vice-versa)\n",
    "peut √™tre extr√™mement pratique car certaines fonctions sont plus ad√©quates sur une forme de donn√©es ou sur l‚Äôautre.\n",
    "\n",
    "En r√®gle g√©n√©rale, avec `Python` comme avec `R`, les **formats *long* sont souvent pr√©f√©rables**.\n",
    "Les formats *wide* sont plut√¥t pens√©s pour des tableurs comme `Excel` ou on dispose d‚Äôun nombre r√©duit\n",
    "de lignes √† partir duquel faire des tableaux crois√©s dynamiques.\n",
    "\n",
    "## 4.2 Exercice d‚Äôapplication\n",
    "\n",
    "Les donn√©es de l‚ÄôADEME, et celles de l‚ÄôInsee √©galement, sont au format\n",
    "*wide*.\n",
    "Le prochain exercice illustre l‚Äôint√©r√™t de faire la conversion *long* $\\to$ *wide*\n",
    "avant de faire un graphique avec la m√©thode `plot` vue au chapitre pr√©c√©dent\n",
    "\n",
    "> **Exercice 2: Restructurer les donn√©es : wide to long**\n",
    ">\n",
    "> 1.  Cr√©er une copie des donn√©es de l‚Äô`ADEME` en faisant `df_wide = emissions_wide.copy()`\n",
    ">\n",
    "> 2.  Restructurer les donn√©es au format *long* pour avoir des donn√©es d‚Äô√©missions par secteur en gardant comme niveau d‚Äôanalyse la commune (attention aux autres variables identifiantes).\n",
    ">\n",
    "> 3.  Faire la somme par secteur et repr√©senter graphiquement\n",
    ">\n",
    "> 4.  Garder, pour chaque d√©partement, le secteur le plus polluant\n",
    "\n",
    "# 5. Joindre des donn√©es\n",
    "\n",
    "## 5.1 Principe\n",
    "\n",
    "Nous allons ici nous focaliser sur le cas le plus favorable qui est la situation\n",
    "o√π une information permet d‚Äôapparier de mani√®re exacte deux bases de donn√©es[1].\n",
    "C‚Äôest un besoin quotidien des *data scientists* d‚Äôassocier des informations pr√©sentes dans plusieurs fichiers. Par exemple, dans des bases de donn√©es d‚Äôentreprises, les informations clients (adresse, √¢ge, etc.) seront dans un fichier, les ventes dans un autre et les caract√©ristiques des produits dans un troisi√®me fichier. Afin d‚Äôavoir une base compl√®te mettant en regard toutes ces informations, il sera d√®s lors n√©cessaire de joindre ces trois fichiers sur la base d‚Äôinformations communes.\n",
    "\n",
    "Cette pratique d√©coule du fait que de nombreux syst√®mes d‚Äôinformation prennent la forme d‚Äôun sch√©ma en √©toile:\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png\" alt=\"Illustration du sch√©ma en √©toile (Source: Databricks)\" />\n",
    "<figcaption aria-hidden=\"true\">Illustration du sch√©ma en √©toile (Source: <a href=\"https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png\">Databricks</a>)</figcaption>\n",
    "</figure>\n",
    "\n",
    "Cette structuration de l‚Äôinformation est tr√®s li√©e au mod√®le des tables relationnelles des ann√©es 1980. Aujourd‚Äôhui, il existe des mod√®les de donn√©es plus flexibles o√π l‚Äôinformation est empil√©e dans un *data lake* sans structure *a priori*. N√©anmoins ce mod√®le du sch√©ma en √©toile conserve une pertinence parce qu‚Äôil permet de partager l‚Äôinformation qu‚Äô√† ceux qui en ont besoin laissant le soin √† ceux qui ont besoin de lier des donn√©es entre elles de le faire.\n",
    "\n",
    "Puisque la logique du sch√©ma en √©toile vient historiquement des bases relationnelles, il est naturel qu‚Äôil s‚Äôagisse d‚Äôune approche intrins√®quement li√©e √† la philosophie du SQL, jusque dans le vocabulaire. On parle souvent de jointure de donn√©es, un h√©ritage du terme `JOIN` de SQL, et la mani√®re de d√©crire les jointures (*left join*, *right join*‚Ä¶) est directement issue des instructions SQL associ√©es.\n",
    "\n",
    "On parle g√©n√©ralement de base de gauche et de droite pour illustrer les jointures:\n",
    "\n",
    "![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/join_initial.png)\n",
    "\n",
    "## 5.2 Mise en oeuvre avec `Pandas`\n",
    "\n",
    "En `Pandas`, la m√©thode la plus pratique pour associer des jeux de donn√©es √† partir de caract√©ristiques communes est `merge`. Ses principaux arguments permettent de contr√¥ler le comportement de jointure. Nous allons les explorer de mani√®re visuelle.\n",
    "\n",
    "En l‚Äôoccurrence, pour notre probl√©matique de construction de statistiques\n",
    "sur les √©missions de gaz carbonique, la base de gauche sera le *DataFrame* `emission` et la base de droite le *DataFrame* `filosofi`:\n",
    "\n",
    "[1] Otherwise, we enter the realm of fuzzy matching or probabilistic matching. Fuzzy matching occurs when we no longer have an exact identifier to link two databases but have partially noisy information between two sources to make the connection. For example, in a product database, we might have `Coca Cola 33CL` and in another `Coca Cola canette`, but these names hide the same product. The chapter on [Introduction to Textual Search with ElasticSearch](../../content/modern-ds/elastic.qmd) addresses this issue. Probabilistic matching is another approach. In these, observations in two databases are associated not based on an identifier but on the distance between a set of characteristics in both databases. This technique is widely used in medical statistics or in the evaluation of public policies based on [*propensity score matching*](https://en.wikipedia.org/wiki/Propensity_score_matching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filosofi.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14214b-67aa-454c-ac3f-70f11aa6ec7d",
   "metadata": {},
   "source": [
    "On parle de cl√©(s) de jointure pour nommer la ou les variable(s) n√©cessaire(s) √† la fusion de donn√©es. Ce sont les variables communes aux deux jeux de donn√©es. Il n‚Äôest pas n√©cessaire qu‚Äôelles aient le m√™me nom en revanche elles doivent partager des valeurs communes autrement l‚Äôintersection entre ces deux bases est l‚Äôensemble vide.\n",
    "\n",
    "On peut jouer sur deux dimensions dans la jointure (ceci sera plus clair ensuite avec les exemples graphiques).\n",
    "\n",
    "-   Il existe principalement trois types de fusions: *left join* et *right join* ou un combo des deux selon le type de pivot qu‚Äôon d√©sire mettre en oeuvre.\n",
    "-   Ensuite, il existe deux mani√®res de fusionner les valeurs une fois qu‚Äôon a choisi un pivot: *inner* ou *outer join*. Dans le premier cas, on ne conserve que les observations o√π les cl√©s de jointures sont pr√©sentes dans les deux bases, dans le second on conserve toutes les observations de la cl√© de jointure des variables pivot quitte √† avoir des valeurs manquantes si la deuxi√®me base de donn√©es n‚Äôa pas de telles observations.\n",
    "\n",
    "Dans les exemples ci-dessous, nous allons utiliser les codes communes et les d√©partements comme variables de jointure. En soi, l‚Äôusage du d√©partement n‚Äôest pas n√©cessaire puisqu‚Äôil se d√©duit directement du code commune mais cela permet d‚Äôillustrer le principe des jointures sur plusieurs variables. A noter que le nom de la commune est volontairement mis de c√¥t√© pour effectuer des jointures alors que c‚Äôest une information commune aux deux bases. Cependant, comme il s‚Äôagit d‚Äôun champ textuel, dont le formattage peut suivre une norme diff√©rente dans les deux bases, ce n‚Äôest pas une information fiable pour faire une jointure exacte.\n",
    "\n",
    "Pour illustrer le principe du pivot √† gauche ou √† droite, on va cr√©er deux variables identificatrices de la ligne de nos jeux de donn√©es de gauche et de droite. Cela nous permettra de trouver facilement les lignes pr√©sentes dans un jeu de donn√©es mais pas dans l‚Äôautre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a61adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions = emissions.reset_index(names = ['id_left'])\n",
    "filosofi = filosofi.reset_index(names = ['id_right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5b432-5172-468f-ad4f-af4c251a01ff",
   "metadata": {},
   "source": [
    "### 5.2.1 *Left join*\n",
    "\n",
    "Commen√ßons avec la jointure √† gauche. Comme son nom l‚Äôindique, on va prendre la variable de gauche en pivot:\n",
    "\n",
    "![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/left_join.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946241cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_merged = emissions.merge(\n",
    "  filosofi,\n",
    "  left_on = [\"INSEE commune\", \"dep\"],\n",
    "  right_on = [\"CODGEO\", \"dep\"],\n",
    "  how = \"left\"\n",
    ")\n",
    "left_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e95d1-5e99-41f2-97f3-8176943aad08",
   "metadata": {},
   "source": [
    "Il est recommand√© de toujours expliciter les cl√©s de jointures par le biais des arguments `left_on`, `right_on` ou `on` si les noms de variables sont communs dans les deux bases.\n",
    "Si on a des noms de variables communes entre les bases mais qu‚Äôelles ne sont pas d√©finies comme cl√©s de jointures, celles-ci ne seront pas utilis√©es pour joindre mais seront conserv√©es avec un suffixe qui par d√©faut est `_x` et `_y` (param√©trable par le biais de l‚Äôargument `suffixes`).\n",
    "\n",
    "La syntaxe `Pandas` √©tant directement inspir√©e de SQL, on a une traduction assez transparente de l‚Äôinstruction ci-dessus en SQL:\n",
    "\n",
    "``` sql\n",
    "SELECT *\n",
    "FROM emissions\n",
    "LEFT JOIN filosofi\n",
    "  ON emissions.`INSEE commune` = filosofi.CODGEO\n",
    "  AND emissions.dep = filosofi.dep;\n",
    "```\n",
    "\n",
    "En faisant une jointure √† gauche, on doit en principe avoir autant de lignes que la base de donn√©es √† gauche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_merged.shape[0] == emissions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454cad02-22c5-40c6-9fc0-4f85d0b73a78",
   "metadata": {},
   "source": [
    "Autrement, cela est signe qu‚Äôil y a une cl√© dupliqu√©e √† droite. Gr√¢ce √† notre variable `id_right`, on peut savoir les codes communes √† droite qui n‚Äôexistent pas √† gauche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c060486",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_merged.loc[left_merged['id_right'].isna()].tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b25fa-4b16-4ed5-beec-6d488c417f0c",
   "metadata": {},
   "source": [
    "Cela vient du fait que nous utilisons des donn√©es qui ne sont pas de la m√™me ann√©e de r√©f√©rence du code officiel g√©ographique (2016 vs 2018). Pendant cet intervalle, il y a eu des changements de g√©ographie, notamment des fusions de communes. Par exemple, la commune de Courcouronnes qu‚Äôon a vu ci-dessus peut √™tre retrouv√©e regroup√©e avec Evry dans le jeu de donn√©es filosofi (base de droite):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77415055",
   "metadata": {},
   "outputs": [],
   "source": [
    "filosofi.loc[\n",
    "  filosofi['LIBGEO']\n",
    "  .str.lower()\n",
    "  .str.contains(\"courcouronnes\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75d7a5-c66c-4fdf-90d2-8c9191ada215",
   "metadata": {},
   "source": [
    "Dans un exercice de construction de statistiques publiques, on ne pourrait donc se permettre cette disjonction des ann√©es.\n",
    "\n",
    "### 5.2.2 *Right join*\n",
    "\n",
    "![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/right_join.png)\n",
    "\n",
    "Le principe est le m√™me mais cette fois c‚Äôest la base de droite qui est prise sous forme de pivot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_merged = emissions.merge(\n",
    "  filosofi,\n",
    "  left_on = [\"INSEE commune\", \"dep\"],\n",
    "  right_on = [\"CODGEO\", \"dep\"],\n",
    "  how = \"right\"\n",
    ")\n",
    "right_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e34bc-3b01-4af0-880c-f012b6992b58",
   "metadata": {},
   "source": [
    "L‚Äôinstruction √©quivalente en SQL serait\n",
    "\n",
    "``` sql\n",
    "SELECT *\n",
    "FROM filosofi\n",
    "RIGHT JOIN emissions\n",
    "  ON filosofi.CODGEO = emissions.`INSEE commune`\n",
    "  AND filosofi.dep = emissions.dep;\n",
    "```\n",
    "\n",
    "On peut, comme pr√©c√©demment, v√©rifier la coh√©rence des dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc64200",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_merged.shape[0] == filosofi.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262fe36f-412f-4d0d-8d45-8cd242c95f62",
   "metadata": {},
   "source": [
    "Pour v√©rifier le nombre de lignes des donn√©es Filosofi que nous n‚Äôavons pas dans notre jeu d‚Äô√©missions de gaz carbonique, on peut faire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_merged['id_left'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c39ff-2b12-4152-93ec-e56fa8e9ea86",
   "metadata": {},
   "source": [
    "C‚Äôest un nombre faible. Quelles sont ces observations ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6175297",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_merged.loc[\n",
    "  right_merged['id_left'].isna(),\n",
    "  filosofi.columns.tolist() + emissions.columns.tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ca29c-6544-420f-af53-5bbb8874a552",
   "metadata": {},
   "source": [
    "Il est suprenant de voir que Paris, Lyon et Marseille sont pr√©sents\n",
    "dans la base des statistiques communales mais pas dans celles des √©missions.\n",
    "Pour comprendre pourquoi, recherchons dans nos donn√©es d‚Äô√©missions les observations li√©es √† Marseille:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc454a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions.loc[\n",
    "  emissions[\"Commune\"]\n",
    "  .str.upper()\n",
    "  .str.contains('MARSEILLE-')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91c181-1a53-4e28-b29f-38924baf2b03",
   "metadata": {},
   "source": [
    "Cela vient du fait que le jeu de donn√©es des √©missions de l‚ÄôAdeme propose de l‚Äôinformation sur les arrondissements dans les trois plus grandes villes\n",
    "l√† o√π le jeu de donn√©es de l‚ÄôInsee ne fait pas cette d√©composition.\n",
    "\n",
    "### 5.2.3 *Inner join*\n",
    "\n",
    "![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/inner.png)\n",
    "\n",
    "Il s‚Äôagit du jeu de donn√©es o√π les cl√©s sont retrouv√©es √† l‚Äôintersection des deux tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_merged = emissions.merge(\n",
    "  filosofi,\n",
    "  left_on = [\"INSEE commune\", \"dep\"],\n",
    "  right_on = [\"CODGEO\", \"dep\"],\n",
    "  how = \"inner\"\n",
    ")\n",
    "inner_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66255cf2-1065-46bb-af3f-902bd488a3d5",
   "metadata": {},
   "source": [
    "En SQL, cela donne\n",
    "\n",
    "``` sql\n",
    "SELECT *\n",
    "FROM emissions\n",
    "INNER JOIN filosofi\n",
    "  ON emissions.`INSEE commune` = filosofi.CODGEO\n",
    "  AND emissions.dep = filosofi.dep;\n",
    "```\n",
    "\n",
    "Le nombre de lignes dans notre jeu de donn√©es peut √™tre compar√© au jeu de droite et de gauche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_merged.shape[0] == (\n",
    "  left_merged.shape[0] - left_merged['id_right'].isna().sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_merged.shape[0] == (\n",
    "  right_merged.shape[0] - right_merged['id_left'].isna().sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ebe46-9d3f-42e9-b6ff-46c45e9f7fd4",
   "metadata": {},
   "source": [
    "### 5.2.4 *Full join*\n",
    "\n",
    "Le *full join* est un pivot √† gauche puis √† droite pour les informations qui n‚Äôont pas √©t√© trouv√©es\n",
    "\n",
    "![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/full_join.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merged = emissions.merge(\n",
    "  filosofi,\n",
    "  left_on = [\"INSEE commune\", \"dep\"],\n",
    "  right_on = [\"CODGEO\", \"dep\"],\n",
    "  how = \"outer\"\n",
    ")\n",
    "full_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec089b-dae2-4a1b-9bc2-da5c8b8b1be7",
   "metadata": {},
   "source": [
    "Comme d‚Äôhabitude, la traduction en SQL est presque imm√©diate:\n",
    "\n",
    "``` sql\n",
    "SELECT *\n",
    "FROM emissions\n",
    "FULL OUTER JOIN filosofi\n",
    "  ON emissions.`INSEE commune` = filosofi.CODGEO\n",
    "  AND emissions.dep = filosofi.dep;\n",
    "```\n",
    "\n",
    "Cette fois, on a une combinaison de nos trois jeux de donn√©es initiaux:\n",
    "\n",
    "-   Le *inner join* ;\n",
    "-   Le *left join* sur les observations sans cl√© de droite ;\n",
    "-   Le *right join* sur les observations sans cl√© de gauche ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "  full_merged['id_left'].isna().sum() + full_merged['id_right'].isna().sum()\n",
    ") == (\n",
    "  left_merged['id_right'].isna().sum() + right_merged['id_left'].isna().sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7828c6-3063-4188-a2b2-2accaa6910ef",
   "metadata": {},
   "source": [
    "### 5.2.5 En r√©sum√©\n",
    "\n",
    "![](https://external-preview.redd.it/yOLzCR0qSzul2WpjQorxINB0xpU3_N9twmFVsgbGJwQ.jpg?auto=webp&s=4feedc91302ba635b3028a21b98d047def5cdc2b)\n",
    "\n",
    "## 5.3 Exemples d‚Äôidentifiants dans les donn√©es fran√ßaises\n",
    "\n",
    "### 5.3.1 Le Code officiel g√©ographique (COG): l‚Äôidentifiant des donn√©es g√©ographiques\n",
    "\n",
    "Pour les donn√©es g√©ographiques, il existe de nombreux identifiants selon la probl√©matique d‚Äô√©tude.\n",
    "Parmi les besoins principaux, on retrouve le fait d‚Äôapparier des donn√©es g√©ographiques √† partir d‚Äôun identifiant administratif commun. Par exemple, associer deux jeux de donn√©es au niveau communal.\n",
    "\n",
    "Pour cela, l‚Äôidentifiant de r√©f√©rence est le code Insee, issu du [Code officiel g√©ographique (COG)](https://www.insee.fr/fr/information/2560452) que nous utilisons depuis le dernier chapitre et que nous aurons amplement l‚Äôoccasion d‚Äôexploiter au cours des diff√©rents chapitres de ce cours.\n",
    "La g√©ographie administrative √©tant en √©volution perp√©tuelle, la base des code Insee est une base vivante. Le site et les API de l‚ÄôInsee permettent de r√©cup√©rer l‚Äôhistorique d‚Äôapr√®s-guerre afin de pouvoir faire de l‚Äôanalyse g√©ographique sur longue p√©riode.\n",
    "\n",
    "Les codes postaux ne peuvent √™tre consid√©r√©s comme un identifiant : ils peuvent regrouper plusieurs communes ou, au contraire, une m√™me commune peut avoir plusieurs codes postaux. Il s‚Äôagit d‚Äôun syst√®me de gestion de la Poste qui n‚Äôa pas √©t√© construit pour l‚Äôanalyse statistique.\n",
    "\n",
    "Pour se convaincre du probl√®me, √† partir des donn√©es mises √† disposition par La Poste, on peut voir que le code postal 11420 correspond √† 11 communes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00da91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_postaux = pd.read_csv(\n",
    "  \"https://datanova.laposte.fr/data-fair/api/v1/datasets/laposte-hexasmal/raw\",\n",
    "  sep = \";\", encoding = \"latin1\",\n",
    "  dtype = {\"Code_postal\": \"str\", \"#Code_commune_INSEE\": \"str\"}\n",
    ")\n",
    "codes_postaux.loc[codes_postaux['Code_postal'] == \"11420\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7fb5f-7d42-4696-ba01-ea34450bc95f",
   "metadata": {},
   "source": [
    "En anticipant sur les comp√©tences d√©velopp√©es lors des prochains chapitres, nous pouvons repr√©senter le probl√®me sous forme cartographique en prenant l‚Äôexemple de l‚ÄôAude. Le code pour produire la carte des codes communes est donn√© tel quel, il n‚Äôest pas d√©velopp√© car il fait appel √† des concepts et librairies qui seront pr√©sent√©s lors du prochain chapitre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cartiflette --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartiflette import carti_download\n",
    "shp_communes = carti_download(\n",
    "  values = [\"11\"],\n",
    "  crs = 4326,\n",
    "  borders = \"COMMUNE\",\n",
    "  simplification=50,\n",
    "  filter_by=\"DEPARTEMENT\",\n",
    "  source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n",
    "  year=2022)\n",
    "\n",
    "codes_postaux11 = shp_communes.merge(\n",
    "  codes_postaux,\n",
    "  left_on = \"INSEE_COM\",\n",
    "  right_on = \"#Code_commune_INSEE\"\n",
    ")\n",
    "codes_postaux11 = codes_postaux11.dissolve(by = \"Code_postal\")\n",
    "\n",
    "# Carte\n",
    "ax = shp_communes.plot(color='white', edgecolor='blue', linewidth = 0.5)\n",
    "ax = codes_postaux11.plot(ax = ax, color='none', edgecolor='black')\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ec721-1d5f-401d-810e-d6e14acaae29",
   "metadata": {},
   "source": [
    "### 5.3.2 Sirene: l‚Äôidentifiant dans les donn√©es d‚Äôentreprises\n",
    "\n",
    "Pour relier les microdonn√©es d‚Äôentreprises fran√ßaises, il existe un num√©ro unique d‚Äôidentification : le [num√©ro `Siren`](https://entreprendre.service-public.fr/vosdroits/F32135). Il s‚Äôagit d‚Äôun num√©ro d‚Äôidentification dans un r√©pertoire l√©gal d‚Äôentreprise indispensable pour toutes d√©marches juridiques, fiscales‚Ä¶ Pour les entreprises qui poss√®dent plusieurs √©tablissements - par exemple dans plusieurs villes - il existe un identifiant d√©riv√© qui s‚Äôappelle le [`Siret`](https://www.economie.gouv.fr/cedef/numero-siret): aux 9 chiffres du num√©ro Sirene s‚Äôajoutent 5 chiffres d‚Äôidentifications de l‚Äô√©tablissement. D‚Äôailleurs, les administrations publiques sont √©galement concern√©es par le num√©ro Siren: √©tant amen√©es √† effectuer des op√©rations de march√©s (achat de mat√©riel, locations de biens, etc.) elles disposent √©galement d‚Äôun identifiant Siren. Etant inscrits dans des r√©pertoires l√©gaux pour lesquels les citoyens sont publics, les num√©ros Siren et les noms des entreprises associ√©es sont disponibles en *open data*, par exemple sur [annuaire-entreprises.data.gouv.fr/](https://annuaire-entreprises.data.gouv.fr/) pour une recherche ponctuelle, sur [data.gouv.fr](https://www.data.gouv.fr/fr/datasets/base-sirene-des-entreprises-et-de-leurs-etablissements-siren-siret/).\n",
    "\n",
    "Cette base Sirene est une mine d‚Äôinformation, parfois comique, sur les entreprises fran√ßaises. Par exemple, le site [tif.hair/](https://tif.hair/) s‚Äôest amus√© √† r√©pertorier la part des salons de coiffures proposant des jeux de mots dans le nom du salon. Lorsqu‚Äôun entrepreneur d√©clare la cr√©ation d‚Äôune entreprise, il re√ßoit un num√©ro Sirene et un code d‚Äôactivit√© (le [code APE](https://entreprendre.service-public.fr/vosdroits/F33050)) reli√© √† la description qu‚Äôil a d√©clar√© de l‚Äôactivit√© de son entreprise. Ce code permet de classer l‚Äôactivit√© d‚Äôune entreprise dans la [Nomenclature d‚Äôactivit√©s fran√ßaises (NAF)](https://www.insee.fr/fr/information/2406147) ce qui servira √† l‚ÄôInsee pour la publication de statistiques sectorielles. En l‚Äôoccurrence, pour les coiffeurs, le code dans la NAF est [`96.02A`](https://www.insee.fr/fr/metadonnees/nafr2/sousClasse/96.02A?champRecherche=false). Il est possible √† partir de la base disponible en *open data* d‚Äôavoir en quelques lignes de `Python` la liste de tous les coiffeurs puis de s‚Äôamuser √† explorer ces donn√©es (objet du prochain exercice optionnel).\n",
    "\n",
    "L‚Äôexercice suivant, optionnel, propose de s‚Äôamuser √† reproduire de mani√®re simplifi√©e le recensement fait par [tif.hair/](https://tif.hair/)\n",
    "des jeux de mots dans les salons de coiffure. Il permet de pratiquer quelques m√©thodes de manipulation textuelle, en avance de phase sur le chapitre consacr√© aux [expressions r√©guli√®res](../../content/manipulation/04b_regex_TP.qmd).\n",
    "\n",
    "Le jeu de donn√©es de l‚Äôensemble des entreprises √©tant assez volumineux (autour de 4Go en CSV apr√®s d√©compression), il est plus pratique de partir sur un jeu de donn√©es au format `Parquet`, plus optimis√© (plus de d√©tails sur ce format dans le [chapitre d‚Äôapprofondissement](../../content/modern-ds/s3.qmd) qui lui est consacr√©).\n",
    "\n",
    "Pour lire ce type de fichiers de mani√®re optimale, il est conseill√© d‚Äôutiliser la librairie `DuckDB` qui permet de ne consommer que les donn√©es n√©cessaires et non de t√©l√©charger l‚Äôensemble du fichier pour n‚Äôen lire qu‚Äôune partie comme ce serait le cas avec `Pandas` (voir la fin de ce chapitre, section ‚ÄúAller au-del√† de `Pandas`‚Äù). La requ√™te SQL suivante se traduit en langage naturel par l‚Äôinstruction suivante: *‚ÄúA partir du fichier `Parquet`, je ne veux que quelques colonnes du fichier pour les coiffeurs (APE: 96.02A) dont le nom de l‚Äôentreprise (`denominationUsuelleEtablissement`) est renseign√©‚Äù*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "coiffeurs = duckdb.sql(\"\"\"\n",
    "  SELECT\n",
    "    siren, siret, dateDebut, enseigne1Etablissement, activitePrincipaleEtablissement, denominationUsuelleEtablissement\n",
    "  FROM\n",
    "    read_parquet('https://minio.lab.sspcloud.fr/lgaliana/data/sirene2024.parquet')\n",
    "  WHERE\n",
    "    activitePrincipaleEtablissement == '96.02A'\n",
    "    AND\n",
    "    denominationUsuelleEtablissement IS NOT NULL\n",
    "\"\"\")\n",
    "coiffeurs = coiffeurs.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f699c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "coiffeurs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2f01b-29d4-45a0-90c3-1993171b0054",
   "metadata": {},
   "source": [
    "> **Exercice optionnel : les coiffeurs blagueurs**\n",
    ">\n",
    "> Dans cet exercice, nous allons consid√©rer exclusivement la variable `denominationUsuelleEtablissement`.\n",
    ">\n",
    "> 1.  Dans cette base, `[ND]` est un code pour valeur manquante. Comme `Python` n‚Äôa pas de raison de le savoir *a priori* et donc d‚Äôavoir interpr√©t√© ces valeurs comme √©tant manquantes, utiliser la m√©thode `replace` pour remplacer `[ND]` par un champ textuel vide. Recoder √©galement les valeurs manquantes sous forme de champ textuel vide afin d‚Äô√©viter des erreurs ult√©rieures li√©es √† l‚Äôimpossibilit√© d‚Äôappliquer certaines m√©thodes textuelles aux valeurs manquantes.\n",
    "> 2.  Rechercher toutes les observations o√π le terme `tif` appara√Æt en faisant attention √† la capitalisation de la variable. Regarder quelques observations\n",
    "> 3.  A partir de [cet exemple](https://stackoverflow.com/a/23996414/9197726), normaliser les noms des salons en retirant les caract√®res sp√©ciaux et compter les jeux de mots les plus fr√©quents\n",
    "\n",
    "Avec la question 2, on retrouve une liste de jeux de mots assez imaginatifs √† partir du terme `tif`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26efb70f-9979-4939-b3e6-64e5a752b1d6",
   "metadata": {},
   "source": [
    "Voici sous une forme plus interactive l‚Äôensemble des coiffeurs qui poss√®dent les termes `tif` dans le nom de leur entreprise d√©pos√©e dans les donn√©es officielles:\n",
    "\n",
    "Bien s√ªr, pour aller plus loin, il faudrait mieux normaliser les donn√©es, v√©rifier que l‚Äôinformation recherch√©e n‚Äôest pas √† cheval sur plusieurs colonnes et bien s√ªr faire de l‚Äôinspection visuelle pour d√©tecter les jeux de mots cach√©s. Mais d√©j√†, en quelques minutes, on a des statistiques partielles sur le ph√©nom√®ne des coiffeurs blagueurs.\n",
    "\n",
    "### 5.3.3 Le NIR et la question de la confidentialit√© des identifiants individuels\n",
    "\n",
    "En ce qui concerne les individus, il existe un identifiant unique permettant de relier ceux-ci dans diff√©rentes sources de donn√©es : le [NIR](https://www.cnil.fr/fr/definition/nir-numero-dinscription-au-repertoire), aussi connu sous le nom de num√©ro Insee ou num√©ro de s√©curit√© sociale.\n",
    "Ce num√©ro est n√©cessaire √† l‚Äôadministration pour la gestion des droits √† prestations sociales (maladie, vieillesse, famille‚Ä¶). Au-del√† de cette fonction qui peut √™tre utile au quotidien, ce num√©ro est un identifiant individuel unique dans le [R√©pertoire national d‚Äôidentification des personnes physiques (RNIPP)](https://www.insee.fr/fr/metadonnees/definition/c1602).\n",
    "\n",
    "Cet identifiant est principalement pr√©sent dans des bases de gestion, li√©es aux fiches de paie, aux prestations sociales, etc. Cependant, *a contrario* du num√©ro Sirene, celui-ci contient en lui-m√™me plusieurs informations sensibles - en plus d‚Äô√™tre intrins√®quement reli√© √† la probl√©matique sensible des droits √† la s√©curit√© sociale.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://www.ameli.fr/sites/default/files/styles/webp_ckeditor/public/thumbnails/image/infographie_assures-regle-identification-assures.gif.webp?itok=j2owVDrB\" alt=\"Le num√©ro de s√©curit√© sociale (Source: Am√©li)\" />\n",
    "<figcaption aria-hidden=\"true\">Le num√©ro de s√©curit√© sociale (Source: <a href=\"https://www.ameli.fr/assure/droits-demarches/principes/numero-securite-sociale\">Am√©li</a>)</figcaption>\n",
    "</figure>\n",
    "\n",
    "Pour pallier ce probl√®me, a r√©c√©mment √©t√© mis en oeuvre le [code statistique non signifiant (CSNS)](https://www.insee.fr/fr/information/7635825?sommaire=7635842) ou NIR hach√©, un identifiant individuel anonyme non identifiant. L‚Äôobjectif de cet identifiant anonymis√© est de r√©duire la diss√©mination d‚Äôune information personnelle qui permettait certes aux fonctionnaires et chercheurs de relier de mani√®re d√©terministe de nombreuses bases de donn√©es mais donnait une information non indispensable aux analystes sur les personnes en question.\n",
    "\n",
    "## 5.4 Exercices d‚Äôapplication\n",
    "\n",
    "### 5.4.1 Pourquoi a-t-on besoin d‚Äôun code commune quand on a d√©j√† son nom ?\n",
    "\n",
    "Cet exercice va revenir un peu en arri√®re afin de saisir pourquoi nous avons pris comme hypoth√®se ci-dessus que le code commune √©tait la cl√© de jointure.\n",
    "\n",
    "> **Exercice 3: v√©rification des cl√©s de jointure**\n",
    ">\n",
    "> On commence par v√©rifier les dimensions des `DataFrames` et la structure de certaines variables cl√©s.\n",
    "> En l‚Äôoccurrence, les variables fondamentales pour lier nos donn√©es sont les variables communales.\n",
    "> Ici, on a deux variables g√©ographiques: un code commune et un nom de commune.\n",
    ">\n",
    "> 1.  V√©rifier les dimensions des *DataFrames*.\n",
    ">\n",
    "> 2.  Identifier dans `filosofi` les noms de communes qui correspondent √† plusieurs codes communes et s√©lectionner leurs codes. En d‚Äôautres termes, identifier les `LIBGEO` tels qu‚Äôil existe des doublons de `CODGEO` et les stocker dans un vecteur `x` (conseil: faire attention √† l‚Äôindex de `x`).\n",
    ">\n",
    "> On se focalise temporairement sur les observations o√π le libell√© comporte plus de deux codes communes diff√©rents\n",
    ">\n",
    "> -   *Question 3*. Regarder dans `filosofi` ces observations.\n",
    ">\n",
    "> -   *Question 4*. Pour mieux y voir, r√©ordonner la base obtenue par order alphab√©tique.\n",
    ">\n",
    "> -   *Question 5*. D√©terminer la taille moyenne (variable nombre de personnes: `NBPERSMENFISC16`) et quelques statistiques descriptives de ces donn√©es.\n",
    ">     Comparer aux m√™mes statistiques sur les donn√©es o√π libell√©s et codes communes co√Øncident.\n",
    ">\n",
    "> -   *Question 6*. V√©rifier les grandes villes (plus de 100 000 personnes),\n",
    ">     la proportion de villes pour lesquelles un m√™me nom est associ√© √† diff√©rents codes commune.\n",
    ">\n",
    "> -   *Question 7*. V√©rifier dans `filosofi` les villes dont le libell√© est √©gal √† Montreuil.\n",
    ">     V√©rifier √©galement celles qui contiennent le terme *‚ÄòSaint-Denis‚Äô*.\n",
    "\n",
    "Ce petit exercice permet donc de se rassurer car les libell√©s dupliqu√©s\n",
    "sont en fait des noms de commune identiques mais qui ne sont pas dans le m√™me d√©partement.\n",
    "Il ne s‚Äôagit donc pas d‚Äôobservations dupliqu√©es.\n",
    "On peut donc se fier aux codes communes, qui eux sont uniques.\n",
    "\n",
    "### 5.4.2 Calculer une empreinte carbone gr√¢ce √† l‚Äôassociation entre des sources\n",
    "\n",
    "> **Exercice 4: Calculer l‚Äôempreinte carbone par habitant**\n",
    ">\n",
    "> En premier lieu, on va calculer l‚Äôempreinte carbone de chaque commune.\n",
    ">\n",
    "> 1.  Cr√©er une variable `emissions` qui correspond aux √©missions totales d‚Äôune commune\n",
    ">\n",
    "> 2.  Faire une jointure √† gauche entre les donn√©es d‚Äô√©missions et les donn√©es de cadrage[1].\n",
    ">\n",
    "> 3.  Calculer l‚Äôempreinte carbone (√©missions totales / population).\n",
    ">\n",
    "> A ce stade nous pourrions avoir envie d‚Äôaller vers la mod√©lisation pour essayer d‚Äôexpliquer\n",
    "> les d√©terminants de l‚Äôempreinte carbone √† partir de variables communales.\n",
    "> Une approche inf√©rentielle n√©cessite n√©anmoins pour √™tre pertinente de\n",
    "> v√©rifier en amont des statistiques descriptives.\n",
    ">\n",
    "> 1.  Sortir un histogramme en niveau puis en log de l‚Äôempreinte carbone communale.\n",
    ">\n",
    "> Avec une meilleure compr√©hension de nos donn√©es, nous nous rapprochons\n",
    "> de la statistique inf√©rentielle. N√©anmoins, nous avons jusqu‚Äô√† pr√©sent\n",
    "> construit des statistiques univari√©es mais n‚Äôavons pas cherch√© √† comprendre\n",
    "> les r√©sultats en regardant le lien avec d‚Äôautres variables.\n",
    "> Cela nous am√®ne vers la statistique bivari√©e, notamment l‚Äôanalyse des corr√©lations.\n",
    "> Ce travail est important puisque toute mod√©lisation ult√©rieure consistera √†\n",
    "> raffiner l‚Äôanalyse des corr√©lations pour tenir compte des corr√©lations crois√©es\n",
    "> entre multiples facteurs. On propose ici de faire cette analyse\n",
    "> de mani√®re minimale.\n",
    ">\n",
    "> 1.  Regarder la corr√©lation entre les variables de cadrage et l‚Äôempreinte carbone. Certaines variables semblent-elles pouvoir potentiellement influer sur l‚Äôempreinte carbone ?\n",
    "\n",
    "A l‚Äôissue de la question 5, le graphique des corr√©lations est le suivant :\n",
    "\n",
    "# 6. Formatter des tableaux de statistiques descriptives\n",
    "\n",
    "Un *dataframe* `Pandas`\n",
    "est automatiquement mis en forme lorsqu‚Äôil est visualis√© depuis un *notebook* sous forme de table HTML √† la mise en forme minimaliste.\n",
    "Cette mise en forme est pratique pour voir\n",
    "les donn√©es, une t√¢che indispensable pour les *data scientists*\n",
    "mais ne permet pas d‚Äôaller vraiment au-del√†.\n",
    "\n",
    "Dans une phase\n",
    "exploratoire, il peut √™tre pratique d‚Äôavoir un tableau\n",
    "un peu plus complet, int√©grant notamment des visualisations\n",
    "minimalistes, pour mieux conna√Ætre ses donn√©es. Dans la phase\n",
    "finale d‚Äôun projet, lorsqu‚Äôon communique sur un projet, il\n",
    "est avantageux de disposer d‚Äôune visualisation attrative.\n",
    "Pour ces deux besoins, les sorties des *notebooks* sont\n",
    "une r√©ponse peu satisfaisante, en plus de n√©cessiter\n",
    "le *medium* du *notebook* qui peut en rebuter certains.\n",
    "\n",
    "Heureusement, le tout jeune *package* [`great_tables`](https://posit-dev.github.io/great-tables/get-started/) permet, simplement, de mani√®re programmatique, la cr√©ation de tableaux\n",
    "qui n‚Äôont rien √† envier √† des productions manuelles fastidieuses faites dans `Excel`\n",
    "et difficilement r√©pliquables. Ce *package* est un portage en `Python` du *package* [`GT`](https://gt.rstudio.com/).\n",
    "`great_tables` construit des tableaux\n",
    "*html* ce qui offre une grande richesse dans la mise en forme et permet une excellente int√©gration avec [`Quarto`](https://quarto.org/), l‚Äôoutil de publication reproductible d√©velopp√© par\n",
    "L‚Äôexercice suivant proposera de construire un tableau avec\n",
    "ce *package*, pas √† pas.\n",
    "\n",
    "Afin de se concentrer sur la construction du tableau,\n",
    "les pr√©parations de donn√©es √† faire en amont sont donn√©es\n",
    "directement. Nous allons repartir de ce jeu de donn√©es:\n",
    "\n",
    "Pour √™tre s√ªr d‚Äô√™tre en mesure d‚Äôeffectuer le prochain exercice, voici le dataframe n√©cessaire pour celui-ci\n",
    "\n",
    "[1] Ideally, it would be necessary to ensure that this join does not introduce bias. Indeed, since our reference years are not necessarily identical, there may be a mismatch between our two sources. Since the exercise is already long, we will not go down this path. Interested readers can perform such an analysis as an additional exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63974146",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions['emissions'] = emissions.sum(axis = 1, numeric_only = True)\n",
    "\n",
    "emissions_merged = (\n",
    "    emissions.reset_index()\n",
    "    .merge(filosofi, left_on = \"INSEE commune\", right_on = \"CODGEO\")\n",
    ")\n",
    "emissions_merged['empreinte'] = emissions_merged['emissions']/emissions_merged['NBPERSMENFISC16']\n",
    "emissions_merged['empreinte'] = emissions_merged['empreinte'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a97f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_table = (\n",
    "    emissions_merged\n",
    "    .rename(columns={\"dep_y\": \"dep\", \"NBPERSMENFISC16\": \"population\", \"MED16\": \"revenu\"})\n",
    "    .groupby(\"dep\")\n",
    "    .agg({\"empreinte\": \"sum\", \"revenu\": \"median\", \"population\": \"sum\"}) #pas vraiment le revenu m√©dian\n",
    "    .reset_index()\n",
    "    .sort_values(by = \"empreinte\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f3c7c-cc43-4c17-83bd-2702c1303ff1",
   "metadata": {},
   "source": [
    "Dans ce tableau nous allons int√©grer des barres horizontales, √† la mani√®re des exemples pr√©sent√©s [ici](https://posit-dev.github.io/great-tables/examples/). Cela se fait en incluant directement le code *html* dans la colonne du *DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar(prop_fill: float, max_width: int, height: int, color: str = \"green\") -> str:\n",
    "    \"\"\"Create divs to represent prop_fill as a bar.\"\"\"\n",
    "    width = round(max_width * prop_fill, 2)\n",
    "    px_width = f\"{width}px\"\n",
    "    return f\"\"\"\\\n",
    "    <div style=\"width: {max_width}px; background-color: lightgrey;\">\\\n",
    "        <div style=\"height:{height}px;width:{px_width};background-color:{color};\"></div>\\\n",
    "    </div>\\\n",
    "    \"\"\"\n",
    "\n",
    "colors = {'empreinte': \"green\", 'revenu': \"red\", 'population': \"blue\"}\n",
    "\n",
    "for variable in ['empreinte', 'revenu', 'population']:\n",
    "    emissions_table[f'raw_perc_{variable}'] = emissions_table[variable]/emissions_table[variable].max()\n",
    "    emissions_table[f'bar_{variable}'] = emissions_table[f'raw_perc_{variable}'].map(\n",
    "        lambda x: create_bar(x, max_width=75, height=20, color = colors[variable])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa6299-2dd2-44cb-b394-68d12ef5b1cc",
   "metadata": {},
   "source": [
    "Nous ne gardons que les 5 plus petites empreintes carbone, et les cinq plus importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_min = emissions_table.head(5).assign(grp = \"5 d√©partements les moins pollueurs\").reset_index(drop=True)\n",
    "emissions_max = emissions_table.tail(5).assign(grp = \"5 d√©partements les plus pollueurs\").reset_index(drop=True)\n",
    "\n",
    "emissions_table = pd.concat([\n",
    "    emissions_min,\n",
    "    emissions_max\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b7f8f-3dd4-4aae-af98-2cbb69666a14",
   "metadata": {},
   "source": [
    "Enfin, pour pouvoir utiliser quelques fonctions pratiques pour s√©lectionner des colonnes √† partir de motifs, nous allons convertir les donn√©es au format [`Polars`](https://pola.rs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "emissions_table = pl.from_pandas(emissions_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe24f04-2120-4d1a-ab26-7e6415592dbe",
   "metadata": {},
   "source": [
    "> **Exercice 5: Un beau tableau de statistiques descriptives (exercice libre)**\n",
    ">\n",
    "> En prenant comme base ce tableau\n",
    ">\n",
    "> ``` python\n",
    "> GT(emissions_table, groupname_col=\"grp\", rowname_col=\"dep\")\n",
    "> ```\n",
    ">\n",
    "> construire un tableau dans le style de celui ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bb5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from here\n",
    "from great_tables import GT\n",
    "GT(emissions_table, groupname_col=\"grp\", rowname_col=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5560636-7919-4450-929a-6cc7a284dd52",
   "metadata": {},
   "source": [
    "Le tableau √† obtenir:\n",
    "\n",
    "Gr√¢ce √† celui-ci, on peut d√©j√† comprendre que notre d√©finition\n",
    "de l‚Äôempreinte carbone est certainement d√©faillante. Il appara√Æt\n",
    "peu plausible que les habitants du 77 aient une empreinte 500 fois\n",
    "sup√©rieure √† celle de Paris intra-muros. La raison principale ?\n",
    "On n‚Äôest pas sur un concept d‚Äô√©missions √† la consommation mais √† la\n",
    "production, ce qui p√©nalise les espaces industriels ou les espaces\n",
    "avec des a√©roports‚Ä¶\n",
    "\n",
    "Pour aller plus loin sur la construction de tableaux\n",
    "avec `great_tables`, vous pouvez r√©pliquer\n",
    "cet [exercice](https://rgeo.linogaliana.fr/exercises/eval.html)\n",
    "de production de tableaux √©lectoraux\n",
    "que j‚Äôai propos√© pour un cours de `R` avec `gt`, l‚Äô√©quivalent\n",
    "de `great_tables` pour `R`.\n",
    "\n",
    "# 7. `Pandas`: vers la pratique et au-del√†\n",
    "\n",
    "## 7.1 `Pandas` dans une chaine d‚Äôop√©rations\n",
    "\n",
    "En g√©n√©ral, dans un projet, le nettoyage de donn√©es va consister en un ensemble de\n",
    "m√©thodes appliqu√©es √† un `DataFrame` ou alors une `Serie` lorsqu‚Äôon travaille exclusivement sur une colonne.\n",
    "Autrement dit, ce qui est g√©n√©ralement attendu lorsqu‚Äôon fait du `Pandas` c‚Äôest d‚Äôavoir une cha√Æne qui prend un `DataFrame` en entr√©e et ressort ce m√™me `DataFrame` enrichi, ou une version agr√©g√©e de celui-ci, en sortie.\n",
    "\n",
    "Cette mani√®re de proc√©der est le coeur de la syntaxe `dplyr` en `R` mais n‚Äôest pas forc√©ment native en `Pandas` selon les op√©rations qu‚Äôon d√©sire mettre en oeuvre. En effet, la mani√®re naturelle de mettre √† jour un *dataframe* en `Pandas` passe souvent par une syntaxe du type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = [[8000, 1000], [9500, np.nan], [5000, 2000]]\n",
    "df = pd.DataFrame(data, columns=['salaire', 'autre_info'])\n",
    "df['salaire_net'] = df['salaire']*0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5d2ef-638c-4b3a-a80b-5a622f18c0ff",
   "metadata": {},
   "source": [
    "En `SQL` on pourrait directement mettre √† jour notre base de donn√©es avec la nouvelle colonne :\n",
    "\n",
    "``` sql\n",
    "SELECT *, salaire*0.8 AS salaire_net FROM df\n",
    "```\n",
    "\n",
    "L‚Äô√©cosyst√®me du *tidyverse* en `R`, l‚Äô√©quivalent de `Pandas`, fonctionne selon la m√™me logique que SQL de mise √† jour de table. On ferait en effet la commande suivante avec `dplyr`:\n",
    "\n",
    "``` r\n",
    "df %>% mutate(salaire_net = salaire*0.8)\n",
    "```\n",
    "\n",
    "Techniquement on pourrait faire ceci avec un `assign` en `Pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64725b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"salaire_net\", axis = \"columns\")\n",
    "df = df.assign(salaire_net = lambda s: s['salaire']*0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269cf50-0706-4fef-8b15-1764aa44d60d",
   "metadata": {},
   "source": [
    "Cependant cette syntaxe `assign` n‚Äôest pas tr√®s naturelle. Il est n√©cessaire de lui passer une *lambda function* qui attend comme *input* un `DataFrame` l√† o√π on voudrait une colonne. Il ne s‚Äôagit donc pas vraiment d‚Äôune syntaxe lisible et pratique.\n",
    "\n",
    "Il est n√©anmoins possible d‚Äôencha√Æner des op√©rations sur des jeux de donn√©es gr√¢ce aux [*pipes*](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html). Ceux-ci reprennent la m√™me philosophie que celle de `dplyr`, elle-m√™me inspir√©e du *pipe* Linux.\n",
    "Cette approche permettra de rendre plus lisible le code en d√©finissant des fonctions effectuant des op√©rations sur une ou plusieurs colonnes d‚Äôun DataFrame. Le premier argument √† indiquer √† la fonction est le `DataFrame`, les autres sont ceux permettant de contr√¥ler son comportement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_salaire_net(df: pd.DataFrame, col: str, taux: float = 0.8):\n",
    "  df[\"salaire_net\"] = df[col]*taux\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df8513-fae1-41b9-bb65-506fbc093692",
   "metadata": {},
   "source": [
    "Ce qui transforme notre chaine de production en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a504f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "  df\n",
    "  .pipe(calcul_salaire_net, \"salaire\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52965474-fed8-4558-a5f3-12074d09bba2",
   "metadata": {},
   "source": [
    "## 7.2 Quelques limites sur la syntaxe de `Pandas`\n",
    "\n",
    "Il y a un avant et un apr√®s `Pandas` dans l‚Äôanalyse de donn√©es en `Python`. Sans ce *package* √¥ combien pratique `Python`, malgr√© toutes les forces de ce langage, aurait eu du mal √† s‚Äôinstaller dans le paysage de l‚Äôanalyse de donn√©es. Cependant, si `Pandas` propose une syntaxe coh√©rente sur de nombreux aspects, elle n‚Äôest pas parfaite non plus. Les paradigmes plus r√©cents d‚Äôanalyse de donn√©es en `Python` ont d‚Äôailleurs parfois l‚Äôambition de corriger ces imperfections syntaxiques l√†.\n",
    "\n",
    "Parmi les points les plus g√©nants au quoditien il y a le besoin de r√©guli√®rement faire des `reset_index` lorsqu‚Äôon construit des statistiques descriptives. En effet, il peut √™tre dangereux de garder des indices qu‚Äôon ne contr√¥le pas bien car, sans attention de notre part lors des phases de *merge*, ils peuvent √™tre utilis√©s √† mauvais escient par `Pandas` pour joindre les donn√©es ce qui peut provoquer des suprises.\n",
    "\n",
    "`Pandas` est extr√™mement bien fait pour restructurer des donn√©es du format *long* to *wide* ou *wide* to *long*. Cependant, ce n‚Äôest pas la seule mani√®re de restructurer un jeu de donn√©es qu‚Äôon peut vouloir mettre en oeuvre. Il arrive r√©guli√®rement qu‚Äôon d√©sire comparer la valeur d‚Äôune observation √† celle d‚Äôun groupe √† laquelle elle appartient. C‚Äôest notamment particuli√®rement utile dans une phase d‚Äôanalyse des anomalies, valeurs aberrantes ou lors d‚Äôune investigation de d√©tection de fraude. De mani√®re native, en `Pandas`, il faut construire une statistique agr√©g√©e par groupe et refaire un *merge* aux donn√©es initiales par le biais de la variable de groupe. C‚Äôest un petit peu fastidieux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_moyennes = emissions.groupby(\"dep\").agg({\"Agriculture\": \"mean\"}).reset_index()\n",
    "emissions_enrichies = (\n",
    "  emissions\n",
    "  .merge(emissions_moyennes, on = \"dep\", suffixes = ['', '_moyenne_dep'])\n",
    ")\n",
    "emissions_enrichies['relatives'] = emissions_enrichies[\"Agriculture\"]/emissions_enrichies[\"Agriculture_moyenne_dep\"]\n",
    "emissions_enrichies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b27684-3058-430d-92bc-006c9dfaf04e",
   "metadata": {},
   "source": [
    "Dans le *tidyverse*, cette op√©ration en deux temps pourrait √™tre faite en une seule √©tape, ce qui est plus pratique\n",
    "\n",
    "``` r\n",
    "emissions %>%\n",
    "  group_by(dep) %>%\n",
    "  mutate(relatives = Agriculture/mean(Agriculture))\n",
    "```\n",
    "\n",
    "Ce n‚Äôest pas si grave mais cela alourdit la longueur des chaines de traitement faites en `Pandas` et donc la charge de maintenance pour les faire durer dans le temps.\n",
    "\n",
    "De mani√®re plus g√©n√©rale, les cha√Ænes de traitement `Pandas` peuvent √™tre assez verbeuses, car il faut r√©guli√®rement red√©finir le `DataFrame` qu‚Äôon utilise plut√¥t que simplement les colonnes. Par exemple, pour faire un filtre sur les lignes et les colonnes, il faudra faire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdca6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "  emissions\n",
    "  .loc[\n",
    "    (emissions[\"dep\"] == \"12\") & (emissions[\"Routier\"]>500), ['INSEE commune', 'Commune']\n",
    "  ]\n",
    "  .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369e1359-4a61-4d2e-94dd-96e3a1895ba2",
   "metadata": {},
   "source": [
    "En SQL on pourrait se contenter de faire r√©f√©rence aux colonnes dans le filter\n",
    "\n",
    "``` sql\n",
    "SELECT \"INSEE commune\", 'Commune'\n",
    "FROM emissions\n",
    "WHERE dep==\"12\" AND Routier>500\n",
    "```\n",
    "\n",
    "Dans le *tidyverse* (`R`) on pourrait aussi faire ceci simplement\n",
    "\n",
    "``` r\n",
    "df %>%\n",
    "  filter(dep==\"12\", Routier>500) %>%\n",
    "  select(`INSEE commune`, `Commune`)\n",
    "```\n",
    "\n",
    "# 8. Les autres paradigmes\n",
    "\n",
    "Ces deux chapitres ont permis d‚Äôexplorer en profondeur la richesse de l‚Äô√©cosyst√®me `Pandas` qui est un indispensable dans la boite √† outil du *data scientist*. Malgr√© toutes les limites que nous avons pu √©voquer, et les solutions alternatives que nous allons pr√©senter, `Pandas` reste LE *package* central de l‚Äô√©cosyst√®me de la donn√©e avec `Python`. Nous allons voir dans les prochains chapitres son int√©gration native √† l‚Äô√©cosyst√®me `Scikit` pour le *machine learning* ou l‚Äôextension de `Pandas` aux donn√©es spatiales avec `GeoPandas`.\n",
    "\n",
    "Les autres solutions techniques que nous allons ici √©voquer peuvent √™tre pertinentes si on d√©sire traiter des volumes de donn√©es importants ou si on d√©sire utiliser des syntaxes alternatives.\n",
    "\n",
    "Les principales alternatives √† `Pandas` sont [`Polars`](https://pola.rs/), [`DuckDB`](https://duckdb.org/) et [`Spark`](https://spark.apache.org/docs/latest/api/python/index.html). Il existe √©galement [`Dask`](https://www.dask.org/), une librairie pour parall√©liser des traitements √©cris en `Pandas`.\n",
    "\n",
    "## 8.1 `Polars`\n",
    "\n",
    "`Polars` est certainement le paradigme le plus inspir√© de `Pandas`, jusqu‚Äôau choix du nom. La premi√®re diff√©rence fondamentale est dans les couches internes utilis√©es. `Polars` s‚Äôappuie sur l‚Äôimpl√©mentation `Rust` de `Arrow` l√† o√π `Pandas` s‚Äôappuie sur `Numpy` ce qui est facteur de perte de performance. Cela permet √† `Polars` d‚Äô√™tre plus efficace sur de gros volumes de donn√©es, d‚Äôautant que de nombreuses op√©rations sont parall√©lis√©es et reposent sur l‚Äô√©valuation diff√©r√©es (*lazy evaluation*) un principe de programmation qui permet d‚Äôoptimiser les requ√™tes pour ne pas les ex√©cuter dans l‚Äôordre de d√©finition mais dans un ordre logique plus optimal.\n",
    "\n",
    "Une autre force de `Polars` est la syntaxe plus coh√©rente, qui b√©n√©ficie du recul d‚Äôune quinzaine d‚Äôann√©es d‚Äôexistence de `Pandas` et d‚Äôune petite dizaine d‚Äôann√©es de `dplyr` (le *package* de manipulation de donn√©es au sein du paradigme du *tidyverse* en `R`). Pour reprendre l‚Äôexemple pr√©c√©dent, il n‚Äôest plus n√©cessaire de forcer la r√©f√©rence au *DataFrame*, dans une cha√Æne d‚Äôex√©cution toutes les r√©f√©rences ult√©rieures seront faites au regard du *DataFrame* de d√©part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a08d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "emissions_polars = pl.from_pandas(emissions)\n",
    "(\n",
    "  emissions_polars\n",
    "  .filter(pl.col(\"dep\") == \"12\", pl.col(\"Routier\") > 500)\n",
    "  .select('INSEE commune', 'Commune')\n",
    "  .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688f951-e1c3-4c6d-9229-ee379aa7e2e4",
   "metadata": {},
   "source": [
    "Pour d√©couvrir `Polars`, de nombreuses ressources en ligne sont accessibles, notamment [ce *notebook*](https://github.com/InseeFrLab/ssphub/blob/main/post/polars/polars-tuto.ipynb) construit pour le r√©seau des *data scientists* de la statistique publique.\n",
    "\n",
    "## 8.2 `DuckDB`\n",
    "\n",
    "*DuckDB* est le nouveau venu dans l‚Äô√©cosyst√®me de l‚Äôanalyse de donn√©es repoussant les limites des donn√©es pouvant √™tre trait√©es avec `Python` sans passer par des outils *big data* comme `Spark`.\n",
    "*DuckDB* est la quintessence d‚Äôun nouveau paradigme, celui du [*‚ÄúBig data is dead‚Äù*](https://motherduck.com/blog/big-data-is-dead/), o√π on peut traiter des donn√©es de volum√©trie importante sans recourir √† des infrastructures imposantes.\n",
    "\n",
    "Outre sa grande efficacit√©, puisqu‚Äôavec *DuckDB* on peut traiter des donn√©es d‚Äôune volum√©trie sup√©rieure √† la m√©moire vive de l‚Äôordinateur ou du serveur, *DuckDB* pr√©sente l‚Äôavantage de proposer une syntaxe uniforme quelle que soit le langage qui appelle *DuckDB* (`Python`, `R`, `C++` ou `Javascript`). *DuckDB* privil√©gie la syntaxe SQL pour traiter les donn√©es avec de nombreuses fonctions pr√©-implement√©es pour simplifier certaines transformations de donn√©es (par exemple pour les [donn√©es textuelles](https://duckdb.org/docs/sql/functions/char.html), les [donn√©es temporelles](https://duckdb.org/docs/sql/functions/time), etc.).\n",
    "\n",
    "Par rapport √† d‚Äôautres syst√®mes s‚Äôappuyant sur SQL, comme [`PostGreSQL`](https://www.bing.com/search?go=Rechercher&q=PostGreSQL&qs=ds&form=QBRE), `DuckDB` est tr√®s simple d‚Äôinstallation, ce n‚Äôest qu‚Äôune librairie `Python` l√† o√π beaucoup d‚Äôoutils comme `PostGreSQL` n√©cessite une infrastructure adapt√©e.\n",
    "\n",
    "Pour reprendre l‚Äôexemple pr√©c√©dent, on peut utiliser directement le code SQL pr√©c√©dent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "duckdb.sql(\n",
    "  \"\"\"\n",
    "  SELECT \"INSEE commune\", \"Commune\"\n",
    "  FROM emissions\n",
    "  WHERE dep=='12' AND Routier>500\n",
    "  LIMIT 5\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae89ca-879f-4e8d-8c36-476ef3c56f32",
   "metadata": {},
   "source": [
    "Ici la clause `FROM emissions` vient du fait qu‚Äôon peut directement ex√©cuter du SQL depuis un objet `Pandas` par le biais de `DuckDB`. Si on fait la lecture directement dans la requ√™te, celle-ci se complexifie un petit peu mais la logique est la m√™me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3086f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "duckdb.sql(\n",
    "  f\"\"\"\n",
    "  SELECT \"INSEE commune\", \"Commune\"\n",
    "  FROM read_csv_auto(\"{url}\")\n",
    "  WHERE\n",
    "    substring(\"INSEE commune\",1,2)=='12'\n",
    "    AND\n",
    "    Routier>500\n",
    "  LIMIT 5\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d93d1-1fb5-4ed5-b10c-00211d829622",
   "metadata": {},
   "source": [
    "Le rendu du *DataFrame* est l√©g√®rement diff√©rent de `Pandas` car, comme `Polars` et de nombreux syst√®mes de traitement de donn√©es volumineuses, `DuckDB` repose sur l‚Äô√©valuation diff√©r√©e et donc ne pr√©sente en *display* qu‚Äôun √©chantillon de donn√©es.\n",
    "`DuckDB` et `Polars` sont d‚Äôailleurs tr√®s bien int√©gr√©s l‚Äôun √† l‚Äôautre. On peut tr√®s bien faire du SQL sur un objet `Polars` via `DuckDB` ou appliquer des fonctions `Polars` sur un objet initialement lu avec `DuckDB`.\n",
    "\n",
    "L‚Äôun des int√©r√™ts de `DuckDB` est son excellente int√©gration avec l‚Äô√©cosyst√®me `Parquet`, le format de donn√©es d√©j√† mentionn√© qui devient un standard dans le partage de donn√©es (il s‚Äôagit, par exemple, de la pierre angulaire du partage de donn√©es sur la plateforme *HuggingFace*). Pour en savoir plus sur `DuckDB` et d√©couvrir son int√©r√™t pour lire les donn√©es du recensement de la population fran√ßaise, vous pouvez consulter [ce post de blog](https://ssphub.netlify.app/post/parquetrp/).\n",
    "\n",
    "## 8.3 `Spark` et le *big data*\n",
    "\n",
    "`DuckDB` a repouss√© les fronti√®res du *big data* qu‚Äôon peut d√©finir comme le volume de donn√©es √† partir duquel on ne peut plus traiter celles-ci sur une machine sans mettre en oeuvre une strat√©gie de parall√©lisation.\n",
    "\n",
    "N√©anmoins, pour les donn√©es tr√®s volumineuses, `Python` est tr√®s bien arm√© gr√¢ce √† la librairie [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html). Celle-ci est une API en Python pour le langage `Spark`, un langage *big data* bas√© sur Scala. Ce paradigme est construit sur l‚Äôid√©e que les utilisateurs de `Python` y acc√®dent par le biais de *cluster* avec de nombreux noeuds pour traiter la donn√©e de mani√®re parall√®le. Celle-ci sera lue par blocs, qui seront trait√©s en parall√®le en fonction du nombre de noeuds parall√®les. L‚ÄôAPI DataFrame de `Spark` pr√©sente une syntaxe proche de celle des paradigmes pr√©c√©dents avec une ing√©nieurie plus complexe en arri√®re-plan li√©e √† la parall√©lisation native."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3602f21a-06ef-438a-b23d-23332583886b",
   "metadata": {},
   "source": [
    "# Premier pas vers l‚Äôindustrialisation avec les pipelines scikit\n",
    "\n",
    "Lino Galiana  \n",
    "2025-10-06\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">Pour essayer les exemples pr√©sents dans ce tutoriel :</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/modelisation/6_pipeline.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=¬´6_pipeline¬ª&init.personalInit=¬´https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh¬ª&init.personalInitArgs=¬´modelisation%206_pipeline%20correction¬ª\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=¬´6_pipeline¬ª&init.personalInit=¬´https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh¬ª&init.personalInitArgs=¬´modelisation%206_pipeline%20correction¬ª\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//blob/main//notebooks/modelisation/6_pipeline.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "Ce chapitre pr√©sente la premi√®re application\n",
    "d‚Äôune journ√©e de cours que j‚Äôai\n",
    "donn√© √† l‚ÄôUniversit√© Dauphine dans le cadre\n",
    "des *PSL Data Week*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b42da-f89c-4c66-a17f-ef3ad430c9ee",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "\n",
    "D√©rouler les _slides_ associ√©es ci-dessous ou [cliquer ici](https://linogaliana.github.io/dauphine-week-data/#/title-slide)\n",
    "pour les afficher en plein √©cran.\n",
    "\n",
    "</summary>\n",
    "\n",
    "\n",
    "<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode yaml code-with-copy\"><code class=\"sourceCode yaml\"></code><button title=\"Copy to Clipboard\" class=\"code-copy-button\"><i class=\"bi\"></i></button></pre><iframe class=\"sourceCode yaml code-with-copy\" src=\"https://linogaliana.github.io/dauphine-week-data/#/title-slide\"></iframe></div>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4196ad5-69b0-45b3-b348-77dbb40daf03",
   "metadata": {},
   "source": [
    "Pour lire les donn√©es de mani√®re efficace, nous\n",
    "proposons d‚Äôutiliser le *package* `duckdb`.\n",
    "Pour l‚Äôinstaller, voici la commande :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf06de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398a27e-84c4-428b-b033-849a0aafccc9",
   "metadata": {},
   "source": [
    "# 1. Pourquoi utiliser les *pipelines* ?\n",
    "\n",
    "## 1.1 D√©finitions pr√©alables\n",
    "\n",
    "Ce chapitre nous am√®nera √† explorer plusieurs √©cosyst√®mes, pour lesquels on retrouve quelques buzz-words dont voici les d√©finitions :\n",
    "\n",
    "| Terme | D√©finition |\n",
    "|---------------------------|---------------------------------------------|\n",
    "| *DevOps* | Mouvement en ing√©nierie informatique et une pratique technique visant √† l‚Äôunification du d√©veloppement logiciel (dev) et de l‚Äôadministration des infrastructures informatiques (ops) |\n",
    "| *MLOps* | Ensemble de pratiques qui vise √† d√©ployer et maintenir des mod√®les de machine learning en production de mani√®re fiable et efficace |\n",
    "\n",
    "Ce chapitre fera des r√©f√©rences r√©guli√®res au cours\n",
    "de 3e ann√©e de l‚ÄôENSAE\n",
    "[*‚ÄúMise en production de projets data science‚Äù*](https://ensae-reproductibilite.github.io/website/).\n",
    "\n",
    "## 1.2 Objectif\n",
    "\n",
    "Les chapitres pr√©c√©dents ont permis de montrer des bouts de code\n",
    "√©pars pour entra√Æner des mod√®les ou faire du *preprocessing*.\n",
    "Cette d√©marche est int√©ressante pour t√¢tonner mais risque d‚Äô√™tre co√ªteuse\n",
    "ult√©rieurement s‚Äôil est n√©cessaire d‚Äôajouter une √©tape de *preprocessing*\n",
    "ou de changer d‚Äôalgorithme.\n",
    "\n",
    "Les *pipelines* sont pens√©s pour simplifier la mise en production\n",
    "ult√©rieure d‚Äôun mod√®le de *machine learning*.\n",
    "Ils sont au coeur de la d√©marche de *MLOps* qui est\n",
    "pr√©sent√©e\n",
    "dans le cours de 3e ann√©e de l‚ÄôENSAE\n",
    "de [*‚ÄúMise en production de projets data science‚Äù*](https://ensae-reproductibilite.github.io/website/),\n",
    "qui vise √† simplifier la mise en oeuvre op√©rationnelle de\n",
    "projets utilisant des techniques de *machine learning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9ab54-c9f0-4c5e-a6f0-cfd508b6710d",
   "metadata": {},
   "source": [
    "## 1.3 Les *pipelines* `Scikit`\n",
    "\n",
    "Heureusement, `Scikit` propose un excellent outil pour proposer un cadre\n",
    "g√©n√©ral pour cr√©er une cha√Æne de production *machine learning*. Il\n",
    "s‚Äôagit des\n",
    "[*pipelines*](https://scikit-learn.org/stable/modules/compose.html).\n",
    "Ils pr√©sentent de nombreux int√©r√™ts, parmi lesquels :\n",
    "\n",
    "-   Ils sont tr√®s **pratiques** et **lisibles**. On rentre des donn√©es en entr√©e, on n‚Äôappelle qu‚Äôune seule fois les m√©thodes `fit` et `predict` ce qui permet de s‚Äôassurer une gestion coh√©rente des transformations de variables, par exemple apr√®s l‚Äôappel d‚Äôun `StandardScaler` ;\n",
    "-   La **modularit√©** rend ais√©e la mise √† jour d‚Äôun pipeline et renforce la capacit√© √† le r√©utiliser ;\n",
    "-   Ils permettent de facilement chercher les hyperparam√®tres d‚Äôun mod√®le. Sans *pipeline*, √©crire un code qui fait du *tuning* d‚Äôhyperparam√®tres peut √™tre p√©nible. Avec les *pipelines*, c‚Äôest une ligne de code ;\n",
    "-   La **s√©curit√©** d‚Äô√™tre certain que les √©tapes de preprocessing sont bien appliqu√©es aux jeux de donn√©es d√©sir√©s avant l‚Äôestimation.\n",
    "\n",
    "> **Tip**\n",
    ">\n",
    "> Un des int√©r√™ts des *pipelines* scikit est qu‚Äôils fonctionnent aussi avec\n",
    "> des m√©thodes qui ne sont pas issues de `scikit`.\n",
    ">\n",
    "> Il est possible d‚Äôintroduire un mod√®le de r√©seau de neurone `Keras` dans\n",
    "> un pipeline `scikit`.\n",
    "> Pour introduire un mod√®le √©conom√©trique `statsmodels`\n",
    "> c‚Äôest un peu plus co√ªteux mais nous allons proposer des exemples\n",
    "> qui peuvent servir de mod√®le et qui montrent que c‚Äôest faisable\n",
    "> sans trop de difficult√©.\n",
    "\n",
    "# 2. Comment cr√©er un *pipeline*\n",
    "\n",
    "Un *pipeline* est un encha√Ænement d‚Äôop√©rations qu‚Äôon code en enchainant\n",
    "des pairs *(cl√©, valeur)* :\n",
    "\n",
    "-   la cl√© est le nom du pipeline, cela peut √™tre utile lorsqu‚Äôon va\n",
    "    repr√©senter le *pipeline* sous forme de diagramme acyclique (visualisation DAG)\n",
    "    ou qu‚Äôon veut afficher des informations sur une √©tape\n",
    "-   la valeur repr√©sente la transformation √† mettre en oeuvre dans le *pipeline*\n",
    "    (c‚Äôest-√†-dire, √† l‚Äôexception de la derni√®re √©tape,\n",
    "    mettre en oeuvre une m√©thode `transform` et √©ventuellement une\n",
    "    transformation inverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eea50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
    "pipe = Pipeline(estimators)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd6a45-65a0-4c9a-9f4a-3d859f912702",
   "metadata": {},
   "source": [
    "Au sein d‚Äôune √©tape de *pipeline*, les param√®tres d‚Äôun estimateur\n",
    "sont accessibles avec la notation `<estimator>__<parameter>`.\n",
    "Cela permet de fixer des valeurs pour les arguments des fonctions `scikit`\n",
    "qui sont appel√©es au sein d‚Äôun *pipeline*.\n",
    "C‚Äôest cela qui rendra l‚Äôapproche des pipelines particuli√®rement utile\n",
    "pour la *grid search* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bfb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"reduce_dim__n_components\":[2, 5, 10], \"clf__C\":[0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f9f71-e0ca-4a76-a65f-02645526bbc8",
   "metadata": {},
   "source": [
    "Ces *pipelines* sont initialis√©s sans donn√©es, il s‚Äôagit d‚Äôune structure formelle\n",
    "que nous allons ensuite ajuster en entra√Ænant des mod√®les.\n",
    "\n",
    "## 2.1 Donn√©es utilis√©es\n",
    "\n",
    "Nous allons utiliser les donn√©es\n",
    "de transactions immobili√®res [DVF](https://app.dvf.etalab.gouv.fr/) pour chercher\n",
    "la meilleure mani√®re de pr√©dire, sachant les caract√©ristiques d‚Äôun bien, son\n",
    "prix.\n",
    "\n",
    "Ces donn√©es sont mises √† disposition\n",
    "sur [`data.gouv`](https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/).\n",
    "N√©anmoins, le format csv n‚Äô√©tant pas pratique pour importer des jeux de donn√©es\n",
    "volumineux, nous proposons de privil√©gier la version `Parquet` mise √†\n",
    "disposition par Eric Mauvi√®re sur [`data.gouv`](https://www.data.gouv.fr/fr/datasets/dvf-2022-format-parquet/#/discussions).\n",
    "L‚Äôapproche la plus efficace pour lire ces donn√©es est\n",
    "d‚Äôutiliser `DuckDB` afin de lire le fichier, extraire les colonnes\n",
    "d‚Äôint√©r√™t puis passer √† `Pandas` (pour en savoir plus sur\n",
    "l‚Äôint√©r√™t de `DuckDB` pour lire des fichiers volumineux, vous pouvez\n",
    "consulter [ce post de blog](https://ssphub.netlify.app/post/parquetrp/) ou\n",
    "[celui-ci](https://www.icem7.fr/outils/3-explorations-bluffantes-avec-duckdb-1-interroger-des-fichiers-distants/) √©crit\n",
    "par Eric Mauvi√®re).\n",
    "\n",
    "M√™me si, en soi, les gains de temps sont faibles car `DuckDB` optimise\n",
    "les requ√™tes HTTPS n√©cessaires √† l‚Äôimport des donn√©es, nous proposons\n",
    "de t√©l√©charger les donn√©es pour r√©duire les besoins de bande passante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e043ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://www.data.gouv.fr/fr/datasets/r/56bde1e9-e214-408b-888d-34c57ff005c4\"\n",
    "file_name = \"dvf.parquet\"\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(file_name):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(\"T√©l√©chargement r√©ussi.\")\n",
    "    else:\n",
    "        print(f\"√âchec du t√©l√©chargement. Code d'√©tat : {response.status_code}\")\n",
    "else:\n",
    "    print(f\"Le fichier '{file_name}' existe d√©j√†. Aucun t√©l√©chargement n√©cessaire.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a5099-4e6e-40d4-921c-77eedadd85c6",
   "metadata": {},
   "source": [
    "En premier lieu, puisque cela va faciliter les requ√™tes SQL ult√©rieures, on cr√©e\n",
    "une vue :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "duckdb.sql(f'CREATE OR REPLACE VIEW dvf AS SELECT * FROM read_parquet(\"dvf.parquet\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd9248-0e7f-4af4-b308-5cd4475376b4",
   "metadata": {},
   "source": [
    "Les donn√©es prennent la forme suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2cb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"SELECT * FROM dvf LIMIT 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9e6ba-2471-45db-91a4-f437072c6c9a",
   "metadata": {},
   "source": [
    "Les variables que nous allons conserver sont les suivantes,\n",
    "nous allons les reformater pour la suite de l‚Äôexercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac032a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = [\n",
    "    \"Date mutation\", \"Valeur fonciere\",\n",
    "    'Nombre de lots', 'Code type local',\n",
    "    'Nombre pieces principales'\n",
    "]\n",
    "xvars = \", \".join([f'\"{s}\"' for s in xvars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893802d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations = duckdb.sql(\n",
    "    f'''\n",
    "    SELECT\n",
    "    date_part('month', \"Date mutation\") AS month,\n",
    "    substring(\"Code postal\", 1, 2) AS dep,\n",
    "    {xvars},\n",
    "    COLUMNS('Surface Carrez.*')\n",
    "    FROM dvf\n",
    "    '''\n",
    ").to_df()\n",
    "\n",
    "colonnes_surface = mutations.columns[mutations.columns.str.startswith('Surface Carrez')]\n",
    "mutations.loc[:, colonnes_surface] = mutations.loc[:, colonnes_surface].replace({',': '.'}, regex=True).astype(float).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949bc3c-6181-40cc-81ca-a7a419595554",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> Le fichier `Parquet` mis √† disposition sur `data.gouv` pr√©sente une incoh√©rence de mise en forme de\n",
    "> certaines colonnes √† cause des virgules qui emp√™chent le formattage sous forme de colonne\n",
    "> num√©rique.\n",
    ">\n",
    "> Le code ci-dessus effectue la conversion ad√©quate au niveau de `Pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba80a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ed19b-5fb2-476b-9ed5-83e51e46268d",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "\n",
    "Introduire un effet confinement\n",
    "\n",
    "</summary>\n",
    "\n",
    "Si vous travaillez avec les donn√©es de 2020, n‚Äôoubliez pas\n",
    "d‚Äôint√©grer l‚Äôeffet\n",
    "confinement dans vos mod√®les puisque cela a lourdement\n",
    "affect√© les possibilit√©s de transaction sur cette p√©riode, donc\n",
    "l‚Äôeffet potentiel de certaines variables explicatives du prix.\n",
    "\n",
    "Pour introduire cet effet, vous pouvez cr√©er une variable\n",
    "indicatrice entre les dates en question:\n",
    "\n",
    "``` python\n",
    "mutations['confinement'] = (\n",
    "    mutations['Date mutation']\n",
    "    .between(pd.to_datetime(\"2020-03-17\"), pd.to_datetime(\"2020-05-03\"))\n",
    "    .astype(int)\n",
    ")\n",
    "```\n",
    "\n",
    "Comme nous travaillons sur les donn√©es de 2022,\n",
    "nous pouvons nous passer de cette variable.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f852ee66-5f55-4d9c-a515-055496e877ff",
   "metadata": {},
   "source": [
    "Les donn√©es DVF proposent une observation par transaction.\n",
    "Ces transactions\n",
    "peuvent concerner plusieurs lots. Par exemple, un appartement\n",
    "avec garage et cave comportera trois lots.\n",
    "\n",
    "Pour simplifier,\n",
    "on va cr√©er une variable de surface qui agr√®ge les diff√©rentes informations\n",
    "de surface disponibles dans le jeu de donn√©es.\n",
    "Les agr√©ger revient √† supposer que le mod√®le de fixation des prix est le m√™me\n",
    "entre chaque lot. C‚Äôest une hypoth√®se simplificatrice qu‚Äôune personne plus\n",
    "experte du march√© immobilier, ou qu‚Äôune approche propre de s√©lection\n",
    "de variable pourrait amener √† nier. En effet, les variables\n",
    "en question sont faiblement corr√©l√©es les unes entre elles, √† quelques\n",
    "exceptions pr√®s (<a href=\"#fig-corr-surface\" class=\"quarto-xref\">Figure¬†2.1</a>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = mutations.loc[\n",
    "    :,\n",
    "    mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()\n",
    "]\n",
    "corr.columns = corr.columns.str.replace(\"Carrez du \", \"\")\n",
    "corr = corr.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c755f-038c-4fbc-9a53-ed29ae1936f2",
   "metadata": {},
   "source": [
    "``` python\n",
    "fig, ax = plt.subplots(1)\n",
    "g = sns.heatmap(\n",
    "    corr, ax=ax, \n",
    "    mask=mask,\n",
    "    vmax=.3, center=0,\n",
    "    square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
    "    xticklabels=corr.columns.values,\n",
    "    yticklabels=corr.columns.values, cmap=cmap, annot=True, fmt=\".2f\"\n",
    ")\n",
    "g\n",
    "```\n",
    "\n",
    "Figure¬†2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations['lprix'] = np.log(mutations[\"Valeur fonciere\"])\n",
    "mutations['surface'] = mutations.loc[:, colonnes_surface].sum(axis = 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations['surface'] = mutations.loc[:, mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a34a3-de56-49bd-a120-437b65945752",
   "metadata": {},
   "source": [
    "# 3. Un premier pipeline : *random forest* sur des variables standardis√©es\n",
    "\n",
    "Notre premier *pipeline* va nous permettre d‚Äôint√©grer ensemble:\n",
    "\n",
    "1.  Une √©tape de *preprocessing* avec la standardisation de variables\n",
    "2.  Une √©tape d‚Äôestimation du prix en utilisant un mod√®le de *random forest*\n",
    "\n",
    "Pour le moment, on va prendre comme acquis un certain nombre de variables\n",
    "explicatives (les *features*) et les hyperparam√®tres du mod√®le.\n",
    "\n",
    "L‚Äôalgorithme des *random forest* est une technique statistique bas√©e sur\n",
    "les arbres de d√©cision. Elle a √©t√© d√©finie explicitement par l‚Äôun\n",
    "des pionniers du *machine learning*, Breiman (2001).\n",
    "Il s‚Äôagit d‚Äôune [m√©thode ensembliste](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "puisqu‚Äôelle consiste √† utiliser plusieurs algorithmes (en l‚Äôoccurrence des arbres\n",
    "de d√©cision) pour obtenir une meilleure pr√©diction que ne le permettraient\n",
    "chaque mod√®le isol√©ment.\n",
    "\n",
    "Les *random forest* sont une m√©thode d‚Äôaggr√©gation[1] d‚Äôarbres de d√©cision.\n",
    "On calcule $K$ arbres de d√©cision et en tire, par une m√©thode d‚Äôagr√©gation,\n",
    "une r√®gle de d√©cision moyenne qu‚Äôon va appliquer pour tirer une\n",
    "pr√©diction de nos donn√©es.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*jE1Cb1Dc_p9WEOPMkC95WQ.png)\n",
    "\n",
    "L‚Äôun des int√©r√™ts\n",
    "des *random forest* est qu‚Äôil existe des m√©thodes pour d√©terminer\n",
    "l‚Äôimportance relative de chaque variable dans la pr√©diction.\n",
    "\n",
    "Nous allons ici partir d‚Äôun *random forest* avec des valeurs d‚Äôhyperparam√®tres\n",
    "donn√©es, √† savoir la profondeur de l‚Äôarbre.\n",
    "\n",
    "## 3.1 D√©finition des ensembles *train* et *test*\n",
    "\n",
    "Nous allons donc nous restreindre √† un sous-ensemble de colonnes dans un\n",
    "premier temps.\n",
    "\n",
    "Nous allons √©galement ne conserver que les\n",
    "transactions inf√©rieures √† 5 millions\n",
    "d‚Äôeuros (on anticipe que celles ayant un montant sup√©rieur sont des transactions\n",
    "exceptionnelles dont le m√©canisme de fixation du prix diff√®re)\n",
    "\n",
    "[1] Les *random forest* sont l‚Äôune des principales m√©thodes\n",
    "ensemblistes. Outre cette approche, les plus connues sont\n",
    "le [*bagging* (*boostrap aggregating*)](https://en.wikipedia.org/wiki/Bootstrap_aggregating) et le *boosting*\n",
    "qui consistent √† choisir la pr√©diction √† privil√©gier\n",
    "selon des algorithmes de choix diff√©rens.\n",
    "Par exemple le *bagging* est une technique bas√©e sur le vote majoritaire (Breiman 1996).\n",
    "Cette technique s‚Äôinspire du *bootstrap* qui, en √©conom√©trie,\n",
    "consiste √† r√©-estimer sur *K* sous-√©chantillons\n",
    "al√©atoires des donn√©es un estimateur afin d‚Äôen tirer, par exemple, un intervalle\n",
    "de confiance empirique √† 95%. Le principe du *bagging* est le m√™me. On r√©-estime\n",
    "*K* fois notre estimateur (par exemple un arbre de d√©cision) et propose une\n",
    "r√®gle d‚Äôagr√©gation pour en tirer une r√®gle moyennis√©e et donc une pr√©diction.\n",
    "Le *boosting* fonctionne selon un principe diff√©rent, bas√© sur\n",
    "l‚Äôoptimisation de combinaisons de classifieurs faibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72592e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations2 = mutations.drop(\n",
    "    colonnes_surface.tolist() + [\"Date mutation\", \"lprix\"], # ajouter \"confinement\" si donn√©es 2020\n",
    "    axis = \"columns\"\n",
    "    ).copy()\n",
    "\n",
    "mutations2 = mutations2.loc[mutations2['Valeur fonciere'] < 5e6] #keep only values below 5 millions\n",
    "\n",
    "mutations2.columns = mutations2.columns.str.replace(\" \", \"_\")\n",
    "mutations2  = mutations2.dropna(subset = ['dep','Code_type_local','month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96cb7d-aefa-4751-a916-af4c91241375",
   "metadata": {},
   "source": [
    "Notre *pipeline* va incorporer deux types de variables: les variables\n",
    "cat√©gorielles et les variables num√©riques.\n",
    "Ces diff√©rents types vont b√©n√©ficier d‚Äô√©tapes de *preprocessing*\n",
    "diff√©rentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = mutations2.columns[~mutations2.columns.isin(['dep','Code_type_local', 'month', 'Valeur_fonciere'])].tolist()\n",
    "categorical_features = ['dep','Code_type_local','month']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2639525-42f6-468c-804d-c7a1c88322c4",
   "metadata": {},
   "source": [
    "Au passage, nous avons abandonn√© la variable de code postal pour privil√©gier\n",
    "le d√©partement afin de r√©duire la dimension de notre jeu de donn√©es. Si on voulait\n",
    "vraiment avoir un bon mod√®le, il faudrait faire autrement car le code postal\n",
    "est probablement un tr√®s bon pr√©dicteur du prix d‚Äôun bien, une fois que\n",
    "les caract√©ristiques du bien sont contr√¥l√©es.\n",
    "\n",
    "> **Exercice 1 : D√©coupage des √©chantillons**\n",
    ">\n",
    "> Nous allons stratifier notre √©chantillonage de *train/test* par d√©partement\n",
    "> afin de tenir compte, de mani√®re minimale, de la g√©ographie.\n",
    "> Pour acc√©l√©rer les calculs pour ce tutoriel, nous n‚Äôallons consid√©rer que\n",
    "> 30% des transactions observ√©es sur chaque d√©partement.\n",
    ">\n",
    "> Voici le code pour le faire:\n",
    ">\n",
    "> ``` python\n",
    "> mutations2 = mutations2.groupby('dep').sample(frac = 0.1, random_state = 123)\n",
    "> ```\n",
    ">\n",
    "> Avec la fonction ad√©quate de `Scikit`, faire un d√©coupage de `mutations2`\n",
    "> en *train* et *test sets*\n",
    "> en suivant les consignes suivantes:\n",
    ">\n",
    "> -   20% des donn√©es dans l‚Äô√©chantillon de *test* ;\n",
    "> -   L‚Äô√©chantillonnage est stratifi√© par d√©partements ;\n",
    "> -   Pour avoir des r√©sultats reproductibles, choisir une racine √©gale √† 123.\n",
    "\n",
    "## 3.2 D√©finition du premier *pipeline*\n",
    "\n",
    "Pour commencer, nous allons fixer la taille des arbres de d√©cision avec\n",
    "l‚Äôhyperparam√®tre `max_depth = 2`.\n",
    "\n",
    "Notre *pipeline* va int√©grer les √©tapes suivantes :\n",
    "\n",
    "1.  **Preprocessing** :\n",
    "    -   Les variables num√©riques vont √™tre standardis√©es avec un `StandardScaler`.\n",
    "        Pour cela, nous allons utiliser la liste `numeric_features` d√©finie pr√©c√©demment.\n",
    "    -   Les variables cat√©gorielles vont √™tre explos√©es avec un *one hot encoding*\n",
    "        (m√©thode `OneHotEncoder` de `scikit`)\n",
    "        Pour cela, nous allons utiliser la liste `categorical_features`\n",
    "2.  **Random forest** : nous allons appliquer l‚Äôestimateur *ad hoc* de `Scikit`.\n",
    "\n",
    "> **Exercice 2 : Construction d‚Äôun premier pipeline formel**\n",
    ">\n",
    "> 1.  Initialiser un *random forest* de profondeur 2. Fixer la racine √† 123 pour avoir des r√©sultats reproductibles.\n",
    "> 2.  La premi√®re √©tape du *pipeline* (nommer cette couche *preprocessor*) consiste √† appliquer les √©tapes de *preprocessing* adapt√©es √† chaque type de variables:\n",
    ">     -   Pour les variables num√©riques, appliquer une √©tape d‚Äôimputation √† la moyenne puis standardiser celles-ci\n",
    ">     -   Pour les variables cat√©gorielles, appliquer un [*one hot encoding*](https://en.wikipedia.org/wiki/One-hot)\n",
    "> 3.  Appliquer comme couche de sortie le mod√®le d√©fini plus t√¥t.\n",
    ">\n",
    "> *üí° Il est recommand√© de s‚Äôaider de la documentation de `Scikit`. Si vous avez besoin d‚Äôun indice suppl√©mentaire, consulter le pipeline pr√©sent√© ci-dessous.*\n",
    "\n",
    "A l‚Äôissue de cet exercice, nous devrions obtenir le *pipeline* suivant.\n",
    "\n",
    "Nous avons construit ce pipeline sous forme de couches successives. La couche\n",
    "`randomforest` prendra automatiquement le r√©sultat de la couche `preprocessor`\n",
    "en *input*. La couche `features` permet d‚Äôintroduire de mani√®re relativement\n",
    "simple (quand on a les bonnes m√©thodes) la complexit√© du *preprocessing*\n",
    "sur donn√©es r√©elles dont les types divergent.\n",
    "\n",
    "A cette √©tape, rien n‚Äôa encore √©t√© estim√©.\n",
    "C‚Äôest tr√®s simple √† mettre en oeuvre\n",
    "avec un *pipeline*.\n",
    "\n",
    "> **Exercice 3 : Mise en oeuvre du pipeline**\n",
    ">\n",
    "> 1.  Estimer les param√®tres du mod√®le sur le jeu d‚Äôentra√Ænement\n",
    "> 2.  Observer la mani√®re dont les donn√©es d‚Äôentra√Ænement sont transform√©es\n",
    ">     par l‚Äô√©tape de *preprocessing* avec les m√©thodes ad√©quates sur 4 observations de `X_train`\n",
    ">     tir√©es al√©atoirement\n",
    "> 3.  Utiliser ce mod√®le pour pr√©dire le prix sur l‚Äô√©chantillon de test. A partir de ces quelques pr√©dictions,\n",
    ">     quel semble √™tre le probl√®me ?\n",
    "> 4.  Observer la mani√®re dont ce *preprocessing* peut s‚Äôappliquer sur deux exemples fictifs :\n",
    ">     -   Un appartement (`code_type_local = 2`) dans le 75, vendu au mois de mai, unique lot de la vente avec 3 pi√®ces, faisant 75m¬≤ ;\n",
    ">     -   Une maison (`code_type_local = 1`) dans le 06, vendue en d√©cembre, dans une transaction avec 2 lots. La surface compl√®te est de 180m¬≤ et le bien comporte 6 pi√®ces.\n",
    "> 5.  D√©duire sur ces deux exemples le prix pr√©dit par le mod√®le.\n",
    "> 6.  Calculer et interpr√©ter le RMSE sur l‚Äô√©chantillon de test. Ce mod√®le est-il satisfaisant ?\n",
    "\n",
    "## 3.3 *Variable importance*\n",
    "\n",
    "Les pr√©dictions semblent avoir une assez faible variance, comme si des variables\n",
    "de seuils intervenaient. Nous allons donc devoir essayer de comprendre pourquoi.\n",
    "\n",
    "La *‚Äúvariable importance‚Äù*\n",
    "se r√©f√®re √† la mesure de l‚Äôinfluence de chaque variable d‚Äôentr√©e sur la performance du mod√®le.\n",
    "L‚Äôimpuret√© fait r√©f√©rence √† l‚Äôincertitude ou √† l‚Äôentropie pr√©sente dans un ensemble de donn√©es.\n",
    "Dans le contexte des *random forest*, cette mesure est souvent calcul√©e en √©valuant la r√©duction moyenne de l‚Äôimpuret√© des n≈ìuds de d√©cision caus√©e par une variable sp√©cifique.\n",
    "Cette approche permet de quantifier l‚Äôimportance des variables dans le processus de prise de d√©cision du mod√®le, offrant ainsi des intuitions sur les caract√©ristiques les plus informatives pour la pr√©diction (plus de d√©tails [sur ce blog](https://mljar.com/blog/feature-importance-in-random-forest/)).\n",
    "\n",
    "On ne va repr√©senter, parmi notre ensemble important de colonnes, que celles\n",
    "qui ont une importance non nulle.\n",
    "\n",
    "> **Exercice 4 : Compr√©hension du mod√®le**\n",
    ">\n",
    "> 1.  R√©cup√©rer la *feature importance* directement depuis la couche adapt√©e de votre *pipeline*\n",
    "> 2.  Utiliser le code suivant pour calculer l‚Äôintervalle de confiance de cette mesure d‚Äôimportance:\n",
    ">\n",
    "> ``` python\n",
    "> std = np.std([tree.feature_importances_ for tree in pipe['randomforest'].estimators_], axis=0)\n",
    "> ```\n",
    ">\n",
    "> 1.  Repr√©senter les variables d‚Äôimportance non nulle. Qu‚Äôen concluez-vous ?\n",
    "\n",
    "Le graphique d‚Äôimportance des variables que vous devriez obtenir √† l‚Äôissue\n",
    "de cet exercice est le suivant.\n",
    "\n",
    "Les statistiques obtenues par le biais de cette *variable importance*\n",
    "sont un peu rudimentaires mais permettent d√©j√† de comprendre\n",
    "le probl√®me de notre mod√®le.\n",
    "\n",
    "On voit donc que deux de nos variables d√©terminantes sont des effets fixes\n",
    "g√©ographiques (qui servent √† ajuster de la diff√©rence de prix entre\n",
    "Paris et les Hauts de Seine et le reste de la France), une autre variable\n",
    "est un effet fixe type de bien. Les deux variables qui pourraient introduire\n",
    "de la variabilit√©, √† savoir la surface et, dans une moindre mesure, le\n",
    "nombre de lots, ont une importance moindre.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> Id√©alement, on utiliserait `Yellowbrick` pour repr√©senter l‚Äôimportance des variables\n",
    "> Mais en l‚Äô√©tat actuel du *pipeline* on a beaucoup de variables dont le poids\n",
    "> est nul qui viennent polluer la visualisation. Vous pouvez\n",
    "> consulter la\n",
    "> [documentation de `Yellowbrick` sur ce sujet](https://www.scikit-yb.org/en/latest/api/model_selection/importances.html)\n",
    "\n",
    "Les pr√©dictions peuvent nous sugg√©rer √©galement\n",
    "qu‚Äôil y a un probl√®me.\n",
    "\n",
    "# 4. Restriction du champ du mod√®le\n",
    "\n",
    "Mettre en oeuvre un bon mod√®le de prix au niveau France enti√®re\n",
    "est complexe. Nous allons donc nous restreindre au champ suivant:\n",
    "les appartements dans Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb061d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations_paris = mutations.drop(\n",
    "    colonnes_surface.tolist() + [\"Date mutation\", \"lprix\"], # ajouter \"confinement\" si donn√©es 2020\n",
    "    axis = \"columns\"\n",
    "    ).copy()\n",
    "\n",
    "mutations_paris = mutations_paris.loc[mutations_paris['Valeur fonciere'] < 5e6] #keep only values below 5 millions\n",
    "\n",
    "mutations_paris.columns = mutations_paris.columns.str.replace(\" \", \"_\")\n",
    "mutations_paris  = mutations_paris.dropna(subset = ['dep','Code_type_local','month'])\n",
    "mutations_paris = mutations_paris.loc[mutations_paris['dep'] == \"75\"]\n",
    "mutations_paris = mutations_paris.loc[mutations_paris['Code_type_local'] == 2].drop(['dep','Code_type_local'], axis = \"columns\")\n",
    "mutations_paris.loc[mutations_paris['surface']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8403b-e91b-4f2a-bc73-e66444b08b0a",
   "metadata": {},
   "source": [
    "> **Exercice 4 : Pipeline plus simple**\n",
    ">\n",
    "> Reprendre les codes pr√©c√©dents et reconstruire notre *pipeline* sur\n",
    "> la nouvelle base en mettant en oeuvre une m√©thode de *boosting*\n",
    "> plut√¥t qu‚Äôune for√™t al√©atoire.\n",
    ">\n",
    "> *La correction de cet exercice est apparente pour simplifier les prochaines √©tapes mais essayez de faire celui-ci de vous-m√™me*.\n",
    "\n",
    "A l‚Äôissue de cet exercice, vous devriez avoir des *MDI* proches\n",
    "de celles-ci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "estim-model-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "mutations_paris = mutations.drop(\n",
    "    colonnes_surface.tolist() + [\"Date mutation\", \"lprix\"], # ajouter \"confinement\" si donn√©es 2020\n",
    "    axis = \"columns\"\n",
    "    ).copy()\n",
    "\n",
    "mutations_paris = mutations_paris.loc[mutations_paris['Valeur fonciere'] < 5e6] #keep only values below 5 millions\n",
    "\n",
    "mutations_paris.columns = mutations_paris.columns.str.replace(\" \", \"_\")\n",
    "mutations_paris  = mutations_paris.dropna(subset = ['dep','Code_type_local','month'])\n",
    "mutations_paris = mutations_paris.loc[mutations_paris['dep'] == \"75\"]\n",
    "mutations_paris = mutations_paris.loc[mutations_paris['Code_type_local'] == 2].drop(['dep','Code_type_local', 'Nombre_de_lots'], axis = \"columns\")\n",
    "mutations_paris.loc[mutations_paris['surface']>0]\n",
    "\n",
    "\n",
    "numeric_features = mutations_paris.columns[~mutations_paris.columns.isin(['month', 'Valeur_fonciere'])].tolist()\n",
    "categorical_features = ['month']\n",
    "\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "numeric_pipeline = make_pipeline(\n",
    "  SimpleImputer(),\n",
    "  StandardScaler()\n",
    ")\n",
    "transformer = make_column_transformer(\n",
    "    (numeric_pipeline, numeric_features),\n",
    "    (OneHotEncoder(sparse_output = False, handle_unknown = \"ignore\"), categorical_features))\n",
    "pipe = Pipeline(steps=[('preprocessor', transformer),\n",
    "                      ('boosting', reg)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mutations_paris.drop(\"Valeur_fonciere\", axis = 1),\n",
    "    mutations_paris[[\"Valeur_fonciere\"]].values.ravel(),\n",
    "    test_size = 0.2, random_state = 123\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "pd.DataFrame(\n",
    "    pipe[\"boosting\"].feature_importances_, \n",
    "    index = pipe[:-1].get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b126051d-70b9-40c9-9f8f-e6cd97684782",
   "metadata": {},
   "source": [
    "# 5. Recherche des hyperparam√®tres optimaux avec une validation crois√©e\n",
    "\n",
    "On d√©tecte que le premier mod√®le n‚Äôest pas tr√®s bon et ne nous aidera\n",
    "pas vraiment √† √©valuer de mani√®re fiable l‚Äôappartement de nos r√™ves.\n",
    "\n",
    "On va essayer de voir si notre mod√®le ne serait pas meilleur avec des\n",
    "hyperparam√®tres plus adapt√©s. Apr√®s tout, nous avons choisi par d√©faut\n",
    "la profondeur de l‚Äôarbre mais c‚Äô√©tait un choix au doigt mouill√©.\n",
    "\n",
    "‚ùìÔ∏è Quels sont les hyperparam√®tres qu‚Äôon peut essayer d‚Äôoptimiser ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc59942",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['boosting'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a26300-eb03-4318-a7cd-19553cb9d304",
   "metadata": {},
   "source": [
    "Un [d√©tour par la documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "nous aide √† comprendre ceux sur lesquels on va jouer. Par exemple, il serait\n",
    "absurde de jouer sur le param√®tre `random_state` qui est la racine du g√©n√©rateur\n",
    "pseudo-al√©atoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d586ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat((X_train, X_test), axis=0)\n",
    "Y = np.concatenate([y_train,y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69ca6f-f077-47e6-92ce-f81c6e1ba320",
   "metadata": {},
   "source": [
    "Nous allons nous contenter de jouer sur les param√®tres:\n",
    "\n",
    "-   `n_estimators`: Le nombre d‚Äôarbres de d√©cision que notre for√™t contient\n",
    "-   `max_depth`: La profondeur de chaque arbre\n",
    "\n",
    "Il existe plusieurs mani√®res de faire de la validation crois√©e. Nous allons ici\n",
    "utiliser la *grid search* qui consiste √† estimer et tester le mod√®le sur chaque\n",
    "combinaison d‚Äôune grille de param√®tres et s√©lectionner le couple de valeurs\n",
    "des hyperparam√®tres amenant √† la meilleure pr√©diction. Par d√©faut, `scikit`\n",
    "effectue une *5-fold cross validation*. Nous n‚Äôallons pas changer\n",
    "ce comportement.\n",
    "\n",
    "Comme expliqu√© pr√©c√©demment, les param√®tres s‚Äôappelent sous la forme\n",
    "`<step>__<parameter_name>`\n",
    "\n",
    "La validation crois√©e pouvant √™tre tr√®s consommatrice de temps, nous\n",
    "n‚Äôallons l‚Äôeffectuer que sur un nombre r√©duit de valeurs de notre grille.\n",
    "Il est possible de passer la liste des valeurs √† passer au crible sous\n",
    "forme de liste\n",
    "(comme nous allons le proposer pour l‚Äôargument `max_depth` dans l‚Äôexercice ci-dessous) ou\n",
    "sous forme d‚Äô`array` (comme nous allons le proposer pour l‚Äôargument `n_estimators`) ce qui est\n",
    "souvent pratique pour g√©n√©rer un criblage d‚Äôun intervalle avec `np.linspace`.\n",
    "\n",
    "> **Tip**\n",
    ">\n",
    "> Les estimations sont, par d√©faut, men√©es de mani√®re s√©quentielle (l‚Äôune apr√®s\n",
    "> l‚Äôautre). Nous sommes cependant face √† un probl√®me\n",
    "> *embarassingly parallel*.\n",
    "> Pour gagner en performance, il est recommand√© d‚Äôutiliser l‚Äôargument\n",
    "> `n_jobs=-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "# Parameters of pipelines can be set using ‚Äò__‚Äô separated parameter names:\n",
    "param_grid = {\n",
    "    \"boosting__n_estimators\": np.linspace(5,25, 5).astype(int),\n",
    "    \"boosting__max_depth\": [2,4]\n",
    "}\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time : {int(end_time - start_time)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bcc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b9dc2-7c3d-4057-88e2-e30cca36e197",
   "metadata": {},
   "source": [
    "Toutes les performances sur les ensembles d‚Äô√©chantillons et de test sur la grille\n",
    "d‚Äôhyperparam√®tres sont disponibles dans l‚Äôattribut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354864e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_random_forest = pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768c491-ed41-4596-a95e-0dc00874fc0a",
   "metadata": {},
   "source": [
    "Regardons les r√©sultats moyens pour chaque valeur des hyperparam√®tres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16816fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "g = sns.lineplot(data = perf_random_forest, ax = ax,\n",
    "             x = \"param_boosting__n_estimators\",\n",
    "             y = \"mean_test_score\",\n",
    "             hue = \"param_boosting__max_depth\")\n",
    "g.set(xlabel='Number of estimators', ylabel='Mean score on test sample')\n",
    "g\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0,\n",
    "           title='Depth of trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd521d1-8676-48f9-9192-df6f11fade03",
   "metadata": {},
   "source": [
    "Globalement, √† profondeur d‚Äôarbre donn√©e, le nombre d‚Äôarbres affecte\n",
    "la performance. Changer la profondeur de l‚Äôarbre am√©liore la\n",
    "performance de mani√®re plus marqu√©e.\n",
    "\n",
    "Maintenant, il nous reste √† re-entra√Æner le mod√®le avec ces nouveaux\n",
    "param√®tres sur l‚Äôensemble du jeu de *train* et l‚Äô√©valuer sur l‚Äôensemble\n",
    "du jeu de *test* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_optimal = grid_search.best_estimator_\n",
    "pipe_optimal.fit(X_train, y_train)\n",
    "\n",
    "compar = pd.DataFrame([y_test, pipe_optimal.predict(X_test)]).T\n",
    "compar.columns = ['obs','pred']\n",
    "compar['diff'] = compar.obs - compar.pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c710b-435e-4277-a755-cc86841514fb",
   "metadata": {},
   "source": [
    "On obtient le RMSE suivant :\n",
    "\n",
    "Et si on regarde la qualit√© en pr√©diction:\n",
    "\n",
    "On obtient plus de variance dans la pr√©diction, c‚Äôest d√©j√† un peu mieux.\n",
    "Cependant, cela reste d√©cevant pour plusieurs raisons:\n",
    "\n",
    "-   nous n‚Äôavons pas fait d‚Äô√©tape de s√©lection de variable\n",
    "-   nous n‚Äôavons pas chercher √† d√©terminer si la variable √† pr√©dire la plus\n",
    "    pertinente √©tait le prix ou une transformation de celle-ci\n",
    "    (par exemple le prix au $m^2$)\n",
    "\n",
    "# 6. Prochaine √©tape\n",
    "\n",
    "Nous avons un mod√®le certes perfectible mais fonctionnel.\n",
    "La question qui se pose maintenant c‚Äôest d‚Äôessayer d‚Äôen faire\n",
    "quelque chose au service des utilisateurs. Cela nous am√®ne vers\n",
    "la question de la **mise en production**.\n",
    "\n",
    "Ceci est l‚Äôobjet du prochain chapitre. Il s‚Äôagira d‚Äôune version introductive\n",
    "des enjeux √©voqu√©s dans le cadre du cours de\n",
    "3e ann√©e de [mise en production de projets de *data science*](https://ensae-reproductibilite.github.io/website/).\n",
    "\n",
    "# R√©f√©rences\n",
    "\n",
    "Breiman, Leo. 1996. ¬´¬†Bagging predictors¬†¬ª. *Machine learning* 24: 123‚Äë40.\n",
    "\n",
    "‚Äî‚Äî‚Äî. 2001. ¬´¬†Random forests¬†¬ª. *Machine learning* 45: 5‚Äë32."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

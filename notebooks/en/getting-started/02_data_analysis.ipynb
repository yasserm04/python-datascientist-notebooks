{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8d38f4-9194-4802-9800-b922f2c96e54",
   "metadata": {},
   "source": [
    "# How to deal with a data set\n",
    "\n",
    "Lino Galiana  \n",
    "2025-10-06\n",
    "\n",
    "> **Purpose of this chapter**\n",
    ">\n",
    "> -   Scientific and technical approach to adopt when faced with a new dataset;\n",
    "> -   Discover the main data providers in France and how to access their data;\n",
    "> -   Discuss the ethical issues behind the work of data scientists and researchers in quantitative science\n",
    "\n",
    "To begin working with a database effectively,\n",
    "it‚Äôs essential to ask some common-sense questions\n",
    "and follow a scientific approach, where some steps are quite straightforward.\n",
    "\n",
    "In a data project, the steps can be schematically divided into four main parts:\n",
    "\n",
    "1.  Data retrieval and structuring;\n",
    "2.  Analysis of the data, including the production of descriptive statistics essential for guiding further explorations;\n",
    "3.  Modeling;\n",
    "4.  Finalizing and communicating results from the previous steps or implementing a production pipeline.\n",
    "\n",
    "This course explores these different stages progressively using the comprehensive `Python` ecosystem. Each chapter of the course can be seen as a way to advance through this process.\n",
    "In this chapter, we will focus on some considerations to make before starting each stage.\n",
    "\n",
    "# 1. When retrieving data\n",
    "\n",
    "## 1.1 Considerations to make in advance\n",
    "\n",
    "The phase of constructing your dataset underpins the entire subsequent project.\n",
    "\n",
    "The first question to ask is, *‚ÄúWhat data do I need to address my problem?‚Äù*\n",
    "This problem might be refined depending on needs, but work is generally of higher quality when the problem prompts reflection on the available data rather than the other way around.\n",
    "\n",
    "Next, *‚ÄúWho produces and provides this data?‚Äù*\n",
    "*Are the sources available on the internet reliable?*\n",
    "Government open data sites are generally quite reliable but sometimes allow the archiving of data restructured by third parties rather than official producers. Conversely, on `Kaggle` or `Github`, the source of some datasets is not tracked, making it difficult to trust the quality of the data.\n",
    "\n",
    "Once one or more data sources are identified, *‚ÄúCan I supplement them with additional data?‚Äù*\n",
    "(In this case, be careful to ensure appropriate levels of granularity).\n",
    "\n",
    "## 1.2 Who produces and disseminates data in France?\n",
    "\n",
    "During the phase of searching for datasets, it is essential to know the key players who produce and disseminate data. Here is an overview of the French data dissemination ecosystem.\n",
    "\n",
    "### 1.2.1 Insee and public statistics administrations\n",
    "\n",
    "Firstly, **statistical institutes** like Insee (Institut National de la Statistique et des √âtudes √âconomiques) in France, as well as **ministerial statistical services (SSM)**[1], produce reliable data on various socioeconomic issues. These are aggregated statistics that, for some local data sources, can be very detailed. These statistics are produced through surveys, individual data from administrative files accessible under a [1951 law](https://www.insee.fr/fr/information/1300616) (see the section on data regulation üëáÔ∏è), or through the exploitation of alternative data sources, such as those from private producers.\n",
    "\n",
    "Insee also produces in-depth studies utilizing the data it generates, which are relevant to read when exploring a socioeconomic issue.\n",
    "\n",
    "Among the *best-sellers* of the data sources available on [insee.fr](https://www.insee.fr/fr/accueil) are census data, unemployment figures, inflation rate, GDP, and the names database. All these sources measured by Insee, which are so widely used in public debate, generally have international definitions to allow comparisons over time and space.\n",
    "\n",
    "In this course, we will occasionally use data sources disseminated by Insee to provide contextual data at an aggregated level.\n",
    "\n",
    "### 1.2.2 IGN for geographical datasets\n",
    "\n",
    "IGN (Institut National de l‚ÄôInformation G√©ographique et Foresti√®re) is another major player that produces and disseminates high-quality geographic and cartographic data in France. This data covers various aspects of the national territory, from topographic maps to land use information, and is essential for projects with a geographic dimension.\n",
    "\n",
    "We will frequently use some of the map backgrounds produced by IGN in our spatial analysis chapters.\n",
    "\n",
    "### 1.2.3 Other central administrations and local authorities\n",
    "\n",
    "Unlike public statistical administrations, the rest of the French administration does not primarily aim to disseminate statistical knowledge. However, data can play an important role in the internal processes of these administrations.\n",
    "\n",
    "For example, the DGFiP (Direction G√©n√©rale des Finances Publiques) holds a vast amount of data from French taxpayers‚Äô tax declarations. Unlike INSEE, which uses this data to gain an exhaustive view of economic inequalities or the situation of French businesses, DGFiP focuses on the administrative management aspect and asks questions such as, *‚ÄúDo the resources align with expectations and will they finance the state‚Äôs budget?‚Äù*\n",
    "\n",
    "Local authorities provide a wide range of local data. This data covers various areas within their scope: urban planning, infrastructure, budget‚Ä¶ They are very useful for studies specific to a region or city, complementing local data provided by other actors, including INSEE. For instance, in this course, we will frequently use the [Open Data portal of the City of Paris](https://opendata.paris.fr/pages/home/).\n",
    "\n",
    "> **The [data.gouv](https://www.data.gouv.fr/fr/) portal: a central piece of French open data ecosystem**\n",
    ">\n",
    "> DINUM (Interministerial Digital Department) is a central administration in France responsible for coordinating digital initiatives within the state. It plays a crucial role in disseminating public data through the [data.gouv](https://www.data.gouv.fr/fr/) platform, which centralizes and provides access to thousands of datasets produced by public administrations, thus facilitating their reuse for research, innovation, or public interest projects.\n",
    "\n",
    "### 1.2.4 Contributory and crowd-sourced datasets\n",
    "\n",
    "Initiatives such as OpenStreetMap, Wikidata, and OpenFoodFacts rely on the voluntary contributions of many users to produce and maintain datasets. These projects are particularly useful for obtaining geospatial data, encyclopedic information, or data on consumer products, respectively.\n",
    "\n",
    "### 1.2.5 Data from private sector\n",
    "\n",
    "Due to the digitization of the economy, many companies collect data on their users or customers as part of their activities. This data, often large and varied, can be used for various purposes, including market analysis or behavioral studies. While data exploitation is indeed central to some digital companies (particularly social networks), many actors internally exploit their customer data. In Europe, the regulatory framework since 2018 is the [GDPR (General Data Protection Regulation)](https://www.cnil.fr/fr/comprendre-le-rgpd), which defines the conditions for collecting, storing, and using personal data.\n",
    "\n",
    "Some companies may also make this data, or an aggregated version of it, available through research projects or access via APIs. These can be useful for answering specific questions, provided one remembers that they are produced from a particular customer base and that extrapolation to the general population is not always possible.\n",
    "\n",
    "## 1.3 Data structuring\n",
    "\n",
    "Next comes the phase of formatting and cleaning the retrieved datasets. This step is crucial and is generally the one that requires the most time. For several years, the term *data cleaning* was used. However, this may have implicitly suggested that it was a subordinate task. The concept of *data wrangling* or *feature engineering* is now preferred, highlighting that it is a skill that requires significant expertise.\n",
    "\n",
    "A clean dataset is one where the structure is appropriate and will not cause errors, visible or not, during the analysis phase. As we will define in the early chapters of the [Manipulation section](../../content/manipulation/index.qmd), the ideal structuring horizon is *tidy* data, i.e., organized in a well-structured table.\n",
    "\n",
    "Here are some characteristics of a clean dataset:\n",
    "\n",
    "-   **missing information** is well understood and addressed. `Numpy` and `Pandas` offer formalism on this topic that it is useful to adopt by replacing missing observations with `NaN`. This involves paying attention to how some producers code missing values: some have a tendency to be imaginative with codes for missing values: *‚Äú-999‚Äù*, *‚ÄúXXX‚Äù*, *‚ÄúNA‚Äù*\n",
    "-   **identifier variables** are the same across tables (especially in the case of joins): same format, same categories‚Ä¶\n",
    "-   for **textual variables**, which can be poorly recorded, correct any possible errors (e.g., ‚ÄúRolland Garros‚Äù -\\> ‚ÄúRoland Garros‚Äù)\n",
    "-   create variables that synthesize the information you need\n",
    "-   remove unnecessary elements (empty columns or rows)\n",
    "-   rename columns with understandable names\n",
    "\n",
    "# 2. During descriptive analysis\n",
    "\n",
    "Once the datasets are cleaned, you can more confidently study the information present in the data. This phase and the cleaning phase are not sequential; in reality, you will regularly move from cleaning to some descriptive statistics that reveal issues, and then back to cleaning, etc.\n",
    "\n",
    "Questions to ask to *‚Äúchallenge‚Äù* the dataset:\n",
    "\n",
    "-   Is my sample **representative** of what I am interested in? Having only 2000 municipalities out of 35000 is not necessarily a problem, but it‚Äôs good to have considered the question.\n",
    "-   Are the **orders of magnitude** correct? To assess this, compare your initial descriptive statistics with your internet research. For example, finding that houses sold in France in 2020 average 400 m¬≤ is not a realistic order of magnitude.\n",
    "-   Do I **understand all the variables** in my dataset? Do they ‚Äúbehave‚Äù as expected? At this stage, it can be useful to create a variable dictionary (explaining how they are constructed or calculated). Correlation studies between variables can also be conducted.\n",
    "-   Do I have any **outliers**, i.e., aberrant values for certain individuals? In this case, decide on the treatment (remove them, apply a logarithmic transformation, leave them as is) and justify it well.\n",
    "-   Do I have any **key insights** from my dataset? Do I have surprising results? If so, have I investigated them thoroughly to see if the results still hold or if it‚Äôs due to issues in the dataset construction (poorly cleaned, incorrect variable‚Ä¶)\n",
    "\n",
    "# 3. During modeling\n",
    "\n",
    "At this stage, descriptive analysis should have provided some initial clues on which direction to take your model. A beginner‚Äôs mistake is to dive straight into modeling because it seems like a more advanced skill. This often leads to poor quality analyses: modeling tends to confirm intuitions derived from descriptive analysis. If the latter has not been thoroughly conducted, interpreting model results can become unnecessarily complex.\n",
    "\n",
    "A background in statistics and econometrics helps in developing better intuitions about the results from a model. It is useful to note that other courses in your statistical curriculum (Econometrics 1, Time Series, Surveys, Data Analysis, etc.) can help you find the most appropriate model for your question.\n",
    "\n",
    "An important point to keep in mind is that the method will be guided by the objective and not the other way around. Among the questions to consider:\n",
    "\n",
    "-   Do you want to explain or predict? Depending on your answer to this question, you will not adopt the same scientific approach or algorithms.\n",
    "-   Do you want to classify an item into a category (supervised classification or unsupervised clustering) or predict a numerical value (regression)?\n",
    "\n",
    "Depending on the models you have already encountered in your courses and the questions you wish to address with your dataset, the choice of model will often be quite straightforward.\n",
    "\n",
    "# 4. During the results presentation phase\n",
    "\n",
    "Sharing code on `Github` or `Gitlab` is a strong incentive to produce high-quality code. It is therefore recommended to systematically use these platforms for code sharing. This is actually a mandatory requirement for validating this course.\n",
    "\n",
    "However, quality gains are not the only reason to adopt the use of `Github` or `Gitlab` on a daily basis. The course I teach with Romain Avouac in the third year of ENSAE ([ensae-reproductibilite.github.io/website/](https://ensae-reproductibilite.github.io/website/)) discusses one of the main benefits of using these platforms, namely the ability to automatically provide various deliverables to showcase your work to different audiences.\n",
    "\n",
    "Depending on the target audience, communication will differ. The code may interest those wanting details on the methodology implemented in practice, but it may be a format that is off-putting to other audiences. Dynamic data visualizations will appeal to less data-savvy audiences but are harder to implement than standard charts.\n",
    "\n",
    "> **Caution**\n",
    ">\n",
    "> *Jupyter* notebooks have been very popular in the data science world for sharing work. However, they are not always the best format. Indeed, many notebooks tend to stack blocks of code and text, which makes them hard to read[2].\n",
    ">\n",
    "> For a substantial project, it is better to move as much code as possible into well-structured scripts and have a notebook that calls these scripts to produce outputs. Alternatively, consider using a different format (a dashboard, a website, an interactive app‚Ä¶).\n",
    ">\n",
    "> In the final-year course at ENSAE, [Data Science Project Deployment](https://ensae-reproductibilite.github.io/website/), Romain Avouac and I review alternative methods for code communication and sharing beyond notebooks.\n",
    ">\n",
    "> This course uses notebooks because they are particularly well-suited for learning `Python`. The ability to insert text between code blocks and the interactivity are ideal for teaching purposes. Once you are more comfortable with `Python`, you can move beyond notebooks to executing scripts.\n",
    "\n",
    "# 5. Ethics and responsibility\n",
    "\n",
    "## 5.1 Reproducibility is important\n",
    "\n",
    "Data is a synthetic representation of reality, and the conclusions of certain analyses can have a real impact on people‚Äôs lives. For instance, the erroneous figures presented by Reinhart and Rogoff (2010) were used as theoretical justification for austerity policies that had severe consequences for citizens in crisis-affected countries[3]. In Great Britain, the Covid-19 case counts in 2020, and thus the monitoring of the epidemic, were incomplete due to truncations caused by the use of an inappropriate data storage format (Excel spreadsheet)[4].\n",
    "\n",
    "Another example is the *credit scoring* system implemented in the United States. The following quote from Hurley and Adebayo (2016)‚Äôs article illustrates the consequences and problematic aspects of an automated credit scoring system:\n",
    "\n",
    "> Consumers have limited ability to identify and contest unfair credit\n",
    "> decisions, and little chance to understand what steps they\n",
    "> should take to improve their credit. Recent studies have also\n",
    "> questioned the accuracy of the data used by these tools, in some\n",
    "> cases identifying serious flaws that have a substantial bearing\n",
    "> on lending decisions. Big-data tools may also risk creating a\n",
    "> system of *‚Äúcreditworthinessby association‚Äù* in which consumers‚Äô\n",
    "> familial, religious, social, and other affiliations determine their\n",
    "> eligibility for an affordable loan.\n",
    ">\n",
    "> Hurley and Adebayo (2016)\n",
    "\n",
    "These problems are unfortunately quite structural in research. A team of Princeton researchers has discussed the *‚Äúreproducibility crisis‚Äù* in the field of *machine learning* due to numerous failures to replicate certain studies (Kapoor and Narayanan 2022). As Guinnane (2023) mentions, many studies in economic history rely on unfounded population figures.\n",
    "\n",
    "Some academic journals have decided to implement a more transparent and reproducible approach. The *American Economic Review* (AER), one of the *top 5* economics journals, has a rather proactive policy on the subject thanks to its [*data editor* Lars Vilhuber](https://aeadataeditor.github.io/).\n",
    "\n",
    "## 5.2 Fighting cognitive biases\n",
    "\n",
    "Transparency about the interests and limitations of a method used is therefore important.\n",
    "This research requirement, sometimes forgotten due to the race for innovative results, also deserves to be applied in business or administration.\n",
    "Even without a manifest intention from the person analyzing the data, misinterpretation is always possible.\n",
    "\n",
    "While highlighting a result, it is possible to point out certain limitations. It is important, in research as well as in discussions with others, to be aware of confirmation bias, which consists of only considering information that aligns with our *a priori* beliefs and ignoring information that might contradict them:\n",
    "\n",
    "![](https://s3.amazonaws.com/revue/items/images/005/107/849/original/59df6bbf7a4b2da55d4eebbd37457f47.png?1571180763)\n",
    "\n",
    "Certain data representations should be excluded as cognitive biases can lead to erroneous interpretations[5]. In the field of data visualization, pie charts or radar charts should be excluded because the human eye poorly perceives these circular shapes. For a similar reason, color-filled maps (choropleth maps) can be misleading.\n",
    "Blog posts for [*datawrapper*](https://blog.datawrapper.de/) by Lisa Charlotte Muth or those by Eric Mauvi√®re are excellent resources for learning good and bad practices in visualization (see the [visualization section](../visualisation/index.qmd) of this course for more details).\n",
    "\n",
    "## 5.3 Data regulation\n",
    "\n",
    "The regulatory framework for data protection has evolved in recent years with the **GDPR**. This regulation has helped to better understand that data collection is justified for more or less well-defined purposes. It is important to recognize that data confidentiality is justified to prevent the uncontrolled dissemination of information about individuals. Particularly sensitive data, such as health data, can be more challenging to handle than less sensitive data.\n",
    "\n",
    "In Europe, for example, agents of public statistical services (e.g., Insee or ministerial statistical services) are bound by professional secrecy (Article L121-6 of the General Civil Service Code), which prohibits them from disclosing confidential information they hold as part of their duties, under penalty of sanctions provided for by Article 226-13 of the Penal Code (up to one year in prison and ‚Ç¨15,000 fine). Statistical secrecy, defined in a 1951 law, strengthens this obligation in the case of data held for statistical purposes. It strictly prohibits the communication of individual data or data that could identify individuals, derived from statistical processing, whether these processes come from surveys or databases. Statistical secrecy generally excludes the dissemination of data that could allow the identification of the concerned individuals, both natural and legal persons. This obligation limits the granularity of the information available for dissemination.\n",
    "\n",
    "This strict framework is explained by the legacy of World War II and the desire to avoid a situation where information collection serves a public action based on discrimination between categories of the population.\n",
    "\n",
    "## 5.4 Sharing methods to reproduce an analysis\n",
    "\n",
    "A [recent article in `Nature`](https://www.nature.com/articles/d41586-022-01692-1),\n",
    "which discusses the work of an epidemiologists‚Äô team (Gabelica, Bojƒçiƒá, and Puljak 2022),\n",
    "raises the issue of data access for researchers wanting to reproduce\n",
    "a study. Even in scientific articles where it is mentioned that\n",
    "data can be made available to other researchers, such sharing is rare:\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://media.nature.com/lw800/magazine-assets/d41586-022-01692-1/d41586-022-01692-1_23176470.png\" alt=\"Graph from the Nature article\" />\n",
    "<figcaption aria-hidden=\"true\">Graph from the <em>Nature</em> article</figcaption>\n",
    "</figure>\n",
    "\n",
    "This somewhat concerning observation is confirmed by a recent study\n",
    "by Samuel and Mietchen (2023), which attempted to execute just under\n",
    "30,000 *notebooks* associated with scientific studies. Only 3%\n",
    "of the *notebooks* reproduce the expected results.\n",
    "\n",
    "To share the means of reproducing publications without disseminating\n",
    "potentially confidential data, synthetic datasets are increasingly used. Through\n",
    "*Deep Learning* models, it is possible to generate complex synthetic datasets\n",
    "that reproduce the main characteristics of a dataset while avoiding, if the model\n",
    "has been well-calibrated, the disclosure of individual information.\n",
    "\n",
    "In French administration, source codes are\n",
    "considered administrative documents and can\n",
    "therefore be made available to any citizen upon request to the\n",
    "Commission for Access to Administrative Documents (CADA):\n",
    "\n",
    "> ‚ÄúAdministrative documents, as defined in Titles I, III, and IV of this book, regardless of their date, place of storage, form, and medium, are documents produced or received in the course of their public service mission by the State, local authorities, as well as other public entities or private entities entrusted with such a mission. Such documents include, in particular, files, reports, studies, minutes, statistics, instructions, circulars, notes and ministerial responses, correspondence, opinions, forecasts, **source codes** and decisions.‚Äù\n",
    ">\n",
    "> [Opinion 20230314 - Session of 30/03/2023 of the Commission for Access to Administrative Documents](https://www.cada.fr/20230314)\n",
    "\n",
    "However, the weights of models used by the administration, particularly those\n",
    "of *machine learning* models, are not regulated in the same way ([Opinion 20230314 from CADA](https://www.cada.fr/20230314)).\n",
    "Indeed, as there is always\n",
    "a risk of reverse engineering leading to partial disclosure\n",
    "of training data when sharing a model, models\n",
    "trained on sensitive data (such as the judicial decisions studied\n",
    "in [Opinion 20230314 from CADA](https://www.cada.fr/20230314))\n",
    "are not intended to be shared.\n",
    "\n",
    "## 5.5 Adopting an ecological approach\n",
    "\n",
    "Digital technology constitutes a growing share of\n",
    "greenhouse gas emissions.\n",
    "Currently representing 4% of global CO2 emissions,\n",
    "this share is expected to grow further (Arcep 2019).\n",
    "The field of *data science* is also\n",
    "concerned.\n",
    "\n",
    "The use of increasingly massive data, particularly the creation\n",
    "of monumental text corpora,\n",
    "gathered through scraping, is a primary source\n",
    "of energy expenditure. Likewise, continuously collecting\n",
    "new digital traces requires maintaining functional\n",
    "servers continuously. In addition to this primary source\n",
    "of energy expenditure, training models can take days,\n",
    "even on very powerful architectures. Strubell, Ganesh, and McCallum (2019)\n",
    "estimates that training a state-of-the-art model in the field of\n",
    "NLP requires as much energy as five cars, on average,\n",
    "over their entire life cycle.\n",
    "\n",
    "The increased use of continuous integration, which allows for\n",
    "automated execution of certain scripts or\n",
    "continuous production of deliverables,\n",
    "also leads to significant energy expenditure.\n",
    "Therefore, it is advisable to limit continuous integration\n",
    "to the production of truly new *outputs*.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> For example, this book makes intensive use of\n",
    "> this approach. Nevertheless, to try to limit\n",
    "> the negative effects of continuously producing an\n",
    "> extensive book, only the modified chapters\n",
    "> are generated during the previews implemented for each `pull request` on the `Github` repository.\n",
    "\n",
    "Data scientists need to be aware\n",
    "of the implications of their intensive use of\n",
    "resources and try to minimize their\n",
    "impact. For example, rather than re-estimating\n",
    "an NLP model, the transfer learning method,\n",
    "which allows for transferring learning weights\n",
    "from one model to a new source, helps\n",
    "reduce computational needs.\n",
    "Similarly, it may be useful to understand\n",
    "the impact of excessively long code by\n",
    "converting computation time into\n",
    "greenhouse gas emissions.\n",
    "The [`codecarbon`](https://codecarbon.io/)\n",
    "package provides this solution by adapting the estimate\n",
    "based on the energy mix of the relevant country.\n",
    "Measuring being a prerequisite to awareness and understanding,\n",
    "such initiatives can lead to increased accountability\n",
    "among data scientists and thus allow for\n",
    "better resource sharing.\n",
    "\n",
    "# References\n",
    "\n",
    "Arcep. 2019. ‚ÄúL‚Äôempreinte Carbone Du Num√©rique.‚Äù *Rapport de l‚ÄôArcep*.\n",
    "\n",
    "Gabelica, Mirko, Ru≈æica Bojƒçiƒá, and Livia Puljak. 2022. ‚ÄúMany Researchers Were Not Compliant with Their Published Data Sharing Statement: Mixed-Methods Study.‚Äù *Journal of Clinical Epidemiology*.\n",
    "\n",
    "Guinnane, Timothy W. 2023. ‚ÄúWe Do Not Know the Population of Every Country in the World for the Past Two Thousand Years.‚Äù *The Journal of Economic History* 83 (3): 912‚Äì38.\n",
    "\n",
    "Hurley, Mikella, and Julius Adebayo. 2016. ‚ÄúCredit Scoring in the Era of Big Data.‚Äù *Yale JL & Tech.* 18: 148.\n",
    "\n",
    "Kapoor, Sayash, and Arvind Narayanan. 2022. ‚ÄúLeakage and the Reproducibility Crisis in ML-Based Science.‚Äù arXiv. <https://doi.org/10.48550/ARXIV.2207.07048>.\n",
    "\n",
    "Reinhart, Carmen M, and Kenneth S Rogoff. 2010. ‚ÄúGrowth in a Time of Debt.‚Äù *American Economic Review* 100 (2): 573‚Äì78.\n",
    "\n",
    "Samuel, Sheeba, and Daniel Mietchen. 2023. ‚ÄúComputational Reproducibility of Jupyter Notebooks from Biomedical Publications.‚Äù <https://arxiv.org/abs/2308.07333>.\n",
    "\n",
    "Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. ‚ÄúEnergy and Policy Considerations for Deep Learning in NLP.‚Äù <https://arxiv.org/abs/1906.02243>.\n",
    "\n",
    "[1] The French public statistical service consists of Insee and the 16 ministerial statistical services (SSM). These are the departments of ministries responsible for the production and dissemination of public service data. Unlike other departments to which the SSMs are attached, they are not solely focused on supporting public action operationally but primarily on providing quantitative elements useful for public debate and public action.\n",
    "\n",
    "[2] In the project submission guidelines ([Evaluation section](content/annexes/evaluation.qmd)), we recommend avoiding monolithic notebooks and offer some solutions for this.\n",
    "\n",
    "[3] Reinhart and Rogoff‚Äôs article, ‚Äú*Growth in a Time of Debt*‚Äù, relied on a manually constructed Excel file. A PhD student discovered errors in it and noted that when official figures were substituted, the results no longer had the same degree of validity.\n",
    "\n",
    "[4] It is assumed here that the erroneous message is transmitted without intention to deceive. Manifest manipulation is an even more serious problem.\n",
    "\n",
    "[5] It is assumed here that the erroneous message is transmitted without intention to deceive. Manifest manipulation is an even more serious problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

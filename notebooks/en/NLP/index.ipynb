{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a70b6cc-6628-44b9-b2f0-a16580d253b7",
   "metadata": {},
   "source": [
    "# Part 4: Natural Language Processing (NLP)\n",
    "\n",
    "Lino Galiana  \n",
    "2025-10-06\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "The previous sections focused on acquiring cross-functional skills for working with data. Naturally, we have so far mostly focused on structured data‚Äîmodest in size but already rich in analytical potential. This new section turns to a subject that, at first glance, may seem unlikely to be handled by computers‚Äîa topic of centuries-old philosophical debate, from Plato to Saussure: the richness of **human language**.\n",
    "\n",
    "By drawing an analogy between ‚Äúlanguage‚Äù and ‚Äútongue‚Äù‚Äîthat is, defining the former as the capacity to express and communicate thought through signs, and the latter as the conventional implementation of that capacity‚Äîwe align ourselves with the field of linguistics and treat language as data.\n",
    "This opens the door to statistical and algorithmic analysis. Yet, even if statistical regularities exist, how can computers‚Äîultimately limited to just *0* and *1*‚Äîgrasp such a complex object as language, which takes humans years to understand and master?[1]\n",
    "\n",
    "# 2. Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) refers to the set of techniques that allow computers to understand, analyze, synthesize, and generate human language[2].\n",
    "\n",
    "NLP is a disciplinary field at the intersection of statistics and linguistics, which has experienced significant growth in recent years ‚Äî academically, operationally, and industrially.\n",
    "Some applications of these techniques have become essential in our daily lives, such as search engines, machine translation, and more recently, chatbots, whose development has accelerated rapidly since the launch of `ChatGPT` in December 2022.\n",
    "\n",
    "# 3. Section Summary\n",
    "\n",
    "This part of the course is dedicated to text data analysis with üìñ examples for fun. It serves as a gradual introduction to the topic by focusing on basic concepts necessary for later understanding of more advanced principles and sophisticated techniques[3]. This section mainly covers:\n",
    "\n",
    "-   The challenges of cleaning textual fields\n",
    "    and frequency analysis. This is somewhat *old school* NLP but understanding it is essential to progress further;\n",
    "-   Language modeling using several approaches.\n",
    "\n",
    "Before diving into the topic of *embeddings*, it‚Äôs important to understand the contributions and limitations of concepts like\n",
    "the bag of words or TF-IDF (*term frequency - inverse document frequency*). One of the main benefits of large language models‚Äînamely the richness of their contextual window that allows them to better grasp textual nuances and speaker intentionality‚Äîbecomes clearer when the limitations of traditional NLP are understood.\n",
    "\n",
    "As an introductory perspective, this course focuses on frequency-based approaches, especially the bag-of-words approach, to ease into the later exploration of the Pandora‚Äôs box that is *embeddings*.\n",
    "\n",
    "## 3.1 Text Cleaning and Frequency Analysis\n",
    "\n",
    "`Python` is an excellent tool for text data analysis.\n",
    "Basic methods for transforming textual data or dictionaries, combined with specialized libraries\n",
    "such as `NLTK` and `SpaCy`, make it possible to perform normalization and text data analysis\n",
    "very efficiently. `Python` is much better equipped than `R` for text data analysis.\n",
    "There is a wealth of online resources on this subject, and the best way to learn remains hands-on practice with a corpus to clean.\n",
    "\n",
    "This section first revisits how to structure and clean a textual corpus\n",
    "through the *bag of words* approach.\n",
    "It aims to demonstrate how to turn a corpus into a tool suitable for\n",
    "statistical analysis:\n",
    "\n",
    "-   It first introduces the challenges of text data cleaning\n",
    "    through an analysis of *The Count of Monte Cristo* by Alexandre Dumas\n",
    "    [here](../../content/NLP/01_intro.qmd), which helps to quickly summarize the available information\n",
    "    in a large volume of text data (as illustrated by wordclouds)\n",
    "-   It then offers a series of exercises on text cleaning based on the\n",
    "    works of Edgar Allan Poe, Mary Shelley, and H.P. Lovecraft, aiming to highlight\n",
    "    the specificity of each author‚Äôs vocabulary. These exercises are\n",
    "    available [in the second chapter](../../content/NLP/02_exoclean.qmd) of the section.\n",
    "\n",
    "This frequency-based analysis provides perspective on the nature of text data and recurring issues in dimensionality reduction of natural language corpora. Just as descriptive statistics naturally lead to modeling, this frequency approach typically quickly leads to the desire to identify underlying rules behind our text corpora.\n",
    "\n",
    "## 3.2 Language Modeling\n",
    "\n",
    "The remainder of this section introduces the challenges of language modeling. These are currently very popular due to the success of `ChatGPT`. However, before delving into large language models (LLMs)‚Äîthose neural networks with billions of parameters trained on massive data volumes‚Äîit‚Äôs important to first understand some preliminary modeling techniques.\n",
    "\n",
    "We begin by exploring an alternative approach that takes into account the context in which a word appears. The introduction of\n",
    "*Latent Dirichlet Allocation* (LDA) serves as an opportunity to present document modeling through *topics*. However, this approach has fallen out of favor in comparison to methods related to the concept of *embedding*.\n",
    "\n",
    "Toward the end of this course section, we will introduce the challenge of transforming textual fields\n",
    "into numeric vector forms. To do so, we will present the principle\n",
    "behind `Word2Vec`, which allows us, for instance,\n",
    "despite significant syntactic distance,\n",
    "to identify that semantically, `Man` and `Woman` are closely related.\n",
    "This chapter serves as a bridge to the concept of *embedding*, a major recent revolution in NLP. It enables the comparison of corpora\n",
    "not only by syntactic similarity (e.g., do they share common words?)\n",
    "but also by semantic similarity (e.g., do they share a theme or meaning?). Covering `Word2Vec` will give curious learners a solid foundation to then explore transformer-based models, which are now the benchmark in NLP.\n",
    "\n",
    "# To Go Further\n",
    "\n",
    "Research in the field of NLP is highly active. It is therefore advisable\n",
    "to stay curious and explore additional resources, as no single source\n",
    "can compile all knowledge‚Äîespecially in a field as dynamic as NLP.\n",
    "\n",
    "To deepen the skills discussed in this course, I strongly recommend\n",
    "this [course by `HuggingFace`](https://huggingface.co/course/chapter1/2?fw=pt).\n",
    "\n",
    "To understand the internal architecture of an LLM,\n",
    "this [post by Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)\n",
    "is very helpful.\n",
    "\n",
    "These chapters only scratch the surface of NLP use cases for *data scientists*.\n",
    "For instance,\n",
    "in public statistics, one major NLP use case involves using automatic classification techniques to convert free-text answers in surveys\n",
    "into predefined fields within a nomenclature.\n",
    "This is a specific adaptation to public statistics, a heavy user of standardized nomenclatures, of multi-level classification problems.\n",
    "\n",
    "Here is an example from a project on automated job classification using the PCS (socio-professional categories) typology,\n",
    "based on a model trained with the `Fasttext` library:\n",
    "\n",
    "[1] While writing these lines, I asked `ChatGPT` to tell me a joke.\n",
    "\n",
    "> Why do programmers hate the beach?\n",
    "> Because the sand keeps changing paradigms!\n",
    "\n",
    "Not understanding it, I asked for an explanation. Here‚Äôs what I got:\n",
    "\n",
    "> Of course, I‚Äôd be happy to explain! This joke plays on the double meaning of the word ‚Äúparadigm‚Äù and is a pun involving programming and the beach.\n",
    ">\n",
    "> In programming, a ‚Äúparadigm‚Äù refers to a model or method of programming, such as object-oriented or functional programming. Programmers are often familiar with different paradigms.\n",
    ">\n",
    "> On the other hand, in a beach context, ‚Äúparadigm‚Äù is humorously misused to mean ‚Äútide‚Äù‚Äîalluding to the natural shifting of sand and water.\n",
    ">\n",
    "> The joke plays on the humorous frustration programmers might feel when adapting to new programming paradigms, just like how sand constantly shifts with the tides. By combining both meanings of ‚Äúparadigm,‚Äù the joke creates an absurd and amusing scenario that will resonate with those familiar with programming and beaches.\n",
    "\n",
    "In the end, I found the explanation funnier than the joke itself.\n",
    "\n",
    "[2] The concept of ‚Äúnatural language processing‚Äù has tended to replace the more specific term ‚Äútext mining.‚Äù Text mining refers to the search for ‚Äî or mining of ‚Äî information within a set of texts. While it is one application area of NLP, it is not the only one. Advances in computing power and in the formalization of language have expanded the scope of fields where computational linguistics is applied.\n",
    "\n",
    "[3] For example, the concept of *embedding*‚Äîthe transformation of a text field into a multidimensional numeric vector‚Äîis central in NLP today but is only briefly mentioned here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac2ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de l'appel API : 503 Server Error: Service Temporarily Unavailable for url: https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=data%20scientist"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "activite = \"data scientist\"\n",
    "urlApe = (\n",
    "    \"https://codification-ape-test.lab.sspcloud.fr/\"\n",
    "    f\"predict?nb_echos_max=3&prob_min=0&text_feature={activite}\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # requ√™te\n",
    "    resp = requests.get(urlApe, timeout=10)\n",
    "    resp.raise_for_status()  # l√®ve une erreur si code HTTP != 200\n",
    "    data = resp.json()\n",
    "\n",
    "    # r√©cup√©ration de IC\n",
    "    IC = data.pop(\"IC\", None)\n",
    "\n",
    "    # transformation en DataFrame\n",
    "    df = pd.DataFrame(data.values())\n",
    "    df[\"indice_confiance\"] = IC\n",
    "\n",
    "    print(df)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Erreur lors de l'appel API :\", e)\n",
    "    df = pd.DataFrame()  # DataFrame vide en cas d'√©chec\n",
    "\n",
    "except (ValueError, KeyError) as e:\n",
    "    print(\"Erreur lors du parsing des donn√©es :\", e)\n",
    "    df = pd.DataFrame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

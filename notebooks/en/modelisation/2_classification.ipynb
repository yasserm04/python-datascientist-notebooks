{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ff8a91-8687-4fc1-a2fe-cc5cbdfb7ecc",
   "metadata": {},
   "source": [
    "# Discovering classification with the SVM technique\n",
    "\n",
    "Lino Galiana  \n",
    "2025-10-06\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">If you want to try the examples in this tutorial:</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/en/modelisation/2_classification.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«2_classification»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«en/modelisation%202_classification%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«2_classification»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«en/modelisation%202_classification%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//en/blob/main//notebooks/en/modelisation/2_classification.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "This chapter aims to very briefly introduce the principle of training models in a classification context. The goal is to illustrate the process using an algorithm with an intuitive principle. It seeks to demonstrate some of the concepts discussed in previous chapters, particularly those related to model training. Other courses in your curriculum will allow you to explore additional classification algorithms and the limitations of each technique.\n",
    "\n",
    "## 1.1 Data\n",
    "\n",
    "Ce chapitre utilise toujours le même jeu de données, présenté dans l’[introduction\n",
    "de cette partie](index.qmd) : les données de vote aux élections présidentielles américaines\n",
    "croisées à des variables sociodémographiques.\n",
    "Le code\n",
    "est disponible [sur Github](https://github.com/linogaliana/python-datascientist/blob/main/content/modelisation/get_data.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f35674",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade xlrd #colab bug verson xlrd\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fdfb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/main/content/modelisation/get_data.py'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('getdata.py', 'wb').write(r.content)\n",
    "\n",
    "import getdata\n",
    "votes = getdata.create_votes_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe1f99-880e-476b-82f9-e2bf85d946a5",
   "metadata": {},
   "source": [
    "## 1.2 The SVM Method (*Support Vector Machines*)\n",
    "\n",
    "SVM (*Support Vector Machines*) is part of the traditional toolkit for *data scientists*.\n",
    "The principle of this technique is relatively intuitive thanks to its geometric interpretation.\n",
    "The goal is to find a line, with margins (supports), that best separates the point cloud in our data.\n",
    "Of course, in real life, it is rare to have well-organized point clouds that can be separated by a line. However, an appropriate projection (a kernel) can transform the data to enable separation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d190819b-179a-4f2f-9c2d-6c1b6692bce2",
   "metadata": {},
   "source": [
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_svc_001.png\" alt=\"Iris SVC Plot\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad0c8a-a901-4074-920d-7f3788d8cee8",
   "metadata": {},
   "source": [
    "> **Mathematical formalization**\n",
    ">\n",
    "> SVM is one of the most intuitive *machine learning* methods\n",
    "> due to its simple geometric interpretation. It is also\n",
    "> one of the least complex *machine learning* algorithms in terms of formalization\n",
    "> for practitioners familiar with traditional statistics. This note provides an overview, though it is not essential for understanding this chapter.\n",
    "> In *machine learning*, more than the mathematical details, the key is to build intuitions.\n",
    ">\n",
    "> The goal of SVM, let us recall, is to find a hyperplane that\n",
    "> best separates the different classes. For example, in a two-dimensional space,\n",
    "> it aims to find a line with margins that best divides the space into regions\n",
    "> with homogeneous *labels*.\n",
    ">\n",
    "> Without loss of generality, we can assume the problem involves a probability distribution $\\mathbb{P}(x,y)$ ($\\mathbb{P} \\to \\{-1,1\\}$) that is unknown. The goal of classification is to build an estimator of the ideal decision function that minimizes the probability of error. In other words\n",
    "\n",
    "$$\n",
    "\\theta = \\arg\\min_\\Theta \\mathbb{P}(h_\\theta(X) \\neq y |x)\n",
    "$$\n",
    "\n",
    "The simplest SVMs are linear SVMs. In this case, it is assumed that a linear separator exists that can assign each class based on its sign:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\text{signe}(f_\\theta(x)) ; \\text{ avec } f_\\theta(x) = \\theta^T x + b\n",
    "$$\n",
    "avec $\\theta \\in \\mathbb{R}^p$ et $w \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2363d2e-fd80-4b44-ba4d-9e73d8ecd779",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png\" alt=\"Les SVM dans le cas linéaire\" style=\"width: 40%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533185f4-eb0a-4db2-9255-f4e67b5be7ae",
   "metadata": {},
   "source": [
    "When observations are linearly separable, there is an infinite number of linear decision boundaries separating the two classes. The *“best”* choice is to select the maximum margin that separates the data. The distance between the two margins is $\\frac{2}{||\\theta||}$. Thus, maximizing this distance between two hyperplanes is equivalent to minimizing $||\\theta||^2$ under the constraint $y_i(\\theta^Tx_i + b) \\geq 1$.\n",
    "\n",
    "In the non-linearly separable case, the *hinge loss* $\\max\\big(0,y_i(\\theta^Tx_i + b)\\big)$ allows for linearizing the loss function, resulting in the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\max\\big(0,y_i(\\theta^Tx_i + b)\\big) + \\lambda ||\\theta||^2\n",
    "$$\n",
    "\n",
    "Generalization to the non-linear case involves introducing kernels that transform the coordinate space of the observations.\n",
    "\n",
    "# 2. Application\n",
    "\n",
    "To apply a classification model, we need to find a dichotomous variable. The natural choice is to use the dichotomous variable of a party’s victory or defeat.\n",
    "\n",
    "Even though the Republicans lost in 2020, they won in more counties (less populated ones). We will consider a Republican victory as our *label* 1 and a defeat as *0*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffcbd1-9ca8-48de-9810-4fd7285577ec",
   "metadata": {},
   "source": [
    "> **Exercise 1: First classification algorithm**\n",
    ">\n",
    "> 1.  Create a *dummy* variable called `y` with a value of 1 when the Republicans win.\n",
    "> 2.  Using the ready-to-use function `train_test_split` from the `sklearn.model_selection` library,\n",
    ">     create test samples (20% of the observations) and training samples (80%) with the following *features*:\n",
    ">\n",
    "> ``` python\n",
    "> vars = [\n",
    ">   \"Unemployment_rate_2019\", \"Median_Household_Income_2021\",\n",
    ">   \"Percent of adults with less than a high school diploma, 2018-22\",\n",
    ">   \"Percent of adults with a bachelor's degree or higher, 2018-22\"\n",
    "> ]\n",
    "> ```\n",
    ">\n",
    "> and use the variable `y` as the *label*.\n",
    ">\n",
    "> *Note: You may encounter the following warning:*\n",
    ">\n",
    "> > A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel()\n",
    ">\n",
    "> *Note: To avoid this warning every time you train your model, you can use `DataFrame[['y']].values.ravel()` instead of `DataFrame[['y']]` when preparing your samples.*\n",
    ">\n",
    "> 1.  Train an SVM classifier with a regularization parameter `C = 1`. Examine the following performance metrics: `accuracy`, `f1`, `recall`, and `precision`.\n",
    ">\n",
    "> 2.  Check the confusion matrix: despite seemingly reasonable scores, you should notice a significant issue.\n",
    ">\n",
    "> 3.  Repeat the previous steps using normalized variables. Are the results different?\n",
    ">\n",
    "> 4.  \\[OPTIONAL\\] Perform 5-fold cross-validation to determine the ideal *C* parameter.\n",
    ">\n",
    "> 5.  Change the *x* variables. Use only the previous Democratic vote result (2016) and income. The variables in question are `share_2016_republican` and `Median_Household_Income_2021`. Examine the results, particularly the confusion matrix.\n",
    "\n",
    "We thus obtain a set of training *features* with the following structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ff3a8-26f1-420e-9b48-d723c341bd52",
   "metadata": {},
   "source": [
    "And the associated *labels* are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d083ef8-ecfc-4c3b-a8e1-9ba3f0d8a5ae",
   "metadata": {},
   "source": [
    "At the end of question 3, our classifier completely misses the 0 labels, which are in the minority. One possible reason is the scale of the variables. Income, in particular, has a distribution that can dominate the others in a linear model. Therefore, at a minimum, it is necessary to standardize the variables, which is the focus of question 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc2abb-6b37-4b3b-8368-00a8d8e4dad4",
   "metadata": {},
   "source": [
    "Standardizing the variables ultimately does not bring any improvement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8210e6a9-c9c9-4c80-95d0-294d275a6b46",
   "metadata": {},
   "source": [
    "It is therefore necessary to go further: the problem does not lie in the scale but in the choice of variables. This is why the step of variable selection is crucial and why a chapter is dedicated to it.\n",
    "\n",
    "At the end of question 6, the new classifier should have the following performance:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
